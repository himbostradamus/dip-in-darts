{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 193\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m--> 193\u001b[0m     denoised_image \u001b[39m=\u001b[39m model(input_noise)  \u001b[39m# Pass noise through the model\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     loss \u001b[39m=\u001b[39m criterion(denoised_image, noisy_image)\n\u001b[1;32m    195\u001b[0m     \u001b[39m# compute PSNR\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 161\u001b[0m, in \u001b[0;36mDenoisingDARTS.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m         weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas_normal, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m     s0, s1 \u001b[39m=\u001b[39m s1, cell(s0, s1, weights)\n\u001b[1;32m    162\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_conv(s1)\n\u001b[1;32m    164\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(out)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 124\u001b[0m, in \u001b[0;36mCell.forward\u001b[0;34m(self, s0, s1, weights)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, s0, s1, weights):\n\u001b[0;32m--> 124\u001b[0m     s2 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ops[i](s1, weights) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m s2\n",
      "Cell \u001b[0;32mIn[9], line 124\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, s0, s1, weights):\n\u001b[0;32m--> 124\u001b[0m     s2 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ops[i](s1, weights) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m))\n\u001b[1;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m s2\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 98\u001b[0m, in \u001b[0;36mMixedOp.forward\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, weights):\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39;49m(op(w \u001b[39m*\u001b[39;49m x) \u001b[39mfor\u001b[39;49;00m w, op \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ops))\n",
      "Cell \u001b[0;32mIn[9], line 98\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, weights):\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(op(w \u001b[39m*\u001b[39;49m x) \u001b[39mfor\u001b[39;00m w, op \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ops))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "\n",
    "class Zero(nn.Module):\n",
    "    def __init__(self, stride):\n",
    "        super(Zero, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.stride == 1:\n",
    "            return x.mul(0.)\n",
    "        return x[:, :, ::self.stride, ::self.stride].mul(0.)\n",
    "\n",
    "\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
    "        super(SepConv, self).__init__()\n",
    "        self.op = nn.Sequential(\n",
    "            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),\n",
    "            nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(C_in, affine=affine),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, padding=padding, groups=C_in, bias=False),\n",
    "            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(C_out, affine=affine),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "class FactorizedReduce(nn.Module):\n",
    "    def __init__(self, C_in, C_out, affine=True):\n",
    "        super(FactorizedReduce, self).__init__()\n",
    "        assert C_out % 2 == 0\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n",
    "        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:])], dim=1)\n",
    "        out = self.bn(out)\n",
    "        return out\n",
    "    \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, symbol=None, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "PRIMITIVES = [\n",
    "    'none',\n",
    "    'avg_pool_3x3',\n",
    "    'max_pool_3x3',\n",
    "    'sep_conv_3x3',\n",
    "    'sep_conv_5x5',\n",
    "    'skip_connect',\n",
    "    'channel_attention'\n",
    "]\n",
    "\n",
    "OPS = {\n",
    "    'none': lambda C, stride, affine: Zero(stride),\n",
    "    'avg_pool_3x3': lambda C, stride, affine: nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False),\n",
    "    'max_pool_3x3': lambda C, stride, affine: nn.MaxPool2d(3, stride=stride, padding=1),\n",
    "    'sep_conv_3x3': lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),\n",
    "    'sep_conv_5x5': lambda C, stride, affine: SepConv(C, C, 5, stride, 2, affine=affine),\n",
    "    'skip_connect': lambda C, stride, affine: nn.Identity() if stride == 1 else FactorizedReduce(C, C, affine),\n",
    "    'channel_attention': lambda C, stride, affine: ChannelAttention(C)\n",
    "}\n",
    "\n",
    "class MixedOp(nn.Module):\n",
    "    def __init__(self, C, stride):\n",
    "        super(MixedOp, self).__init__()\n",
    "        self._ops = nn.ModuleList()\n",
    "        for primitive in PRIMITIVES:\n",
    "            op = OPS[primitive](C, stride, False)\n",
    "            if 'pool' in primitive:\n",
    "                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))\n",
    "            self._ops.append(op)\n",
    "\n",
    "    def forward(self, x, weights):\n",
    "        return sum(op(w * x) for w, op in zip(weights, self._ops))\n",
    "\n",
    "\n",
    "\n",
    "class Cell(nn.Module):\n",
    "    multiplier = 1\n",
    "    def __init__(self, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
    "        super(Cell, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "        # Define operations\n",
    "        # For simplicity, let's just assume we have 2 nodes for operations.\n",
    "        self._ops = nn.ModuleList()\n",
    "        for _ in range(2):\n",
    "            op = MixedOp(C, stride=2 if reduction else 1)\n",
    "            self._ops.append(op)\n",
    "\n",
    "        self._initialize_alphas()\n",
    "\n",
    "    def _initialize_alphas(self):\n",
    "        k = sum(1 for i in range(2))\n",
    "        num_ops = len(PRIMITIVES)\n",
    "        self.alphas_normal = nn.Parameter(1e-3 * torch.randn(k, num_ops))\n",
    "        self.alphas_reduce = nn.Parameter(1e-3 * torch.randn(k, num_ops))\n",
    "\n",
    "    def forward(self, s0, s1, weights):\n",
    "        s2 = sum(self._ops[i](s1, weights) for i in range(2))\n",
    "        return s2\n",
    "    \n",
    "class DenoisingDARTS(nn.Module):\n",
    "    def __init__(self, C=16, num_cells=3):\n",
    "        super(DenoisingDARTS, self).__init__()\n",
    "        self.C = C\n",
    "        self.num_cells = num_cells\n",
    "\n",
    "        # Initial convolution\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, C, 3, padding=1),\n",
    "            nn.BatchNorm2d(C)\n",
    "        )\n",
    "\n",
    "        C_prev_prev, C_prev, C_curr = C, C, C\n",
    "\n",
    "        self.alphas_normal = nn.Parameter(torch.randn(1, len(OPS)))\n",
    "        self.alphas_reduce = nn.Parameter(torch.randn(1, len(OPS)))\n",
    "\n",
    "        self.cells = nn.ModuleList()\n",
    "        reduction_prev = False\n",
    "        for i in range(num_cells):\n",
    "            cell = Cell(C_prev_prev, C_prev, C_curr, reduction=False, reduction_prev=reduction_prev)\n",
    "            self.cells += [cell]\n",
    "            C_prev_prev, C_prev = C_prev, cell.multiplier * C_curr\n",
    "            reduction_prev = False\n",
    "\n",
    "        self.output_conv = nn.Conv2d(C_prev, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s0 = s1 = self.stem(x)\n",
    "        for i, cell in enumerate(self.cells):\n",
    "            if cell.reduction:\n",
    "                weights = F.softmax(self.alphas_reduce, dim=-1)\n",
    "            else:\n",
    "                weights = F.softmax(self.alphas_normal, dim=-1)\n",
    "            s0, s1 = s1, cell(s0, s1, weights)\n",
    "        out = self.output_conv(s1)\n",
    "\n",
    "        return torch.sigmoid(out) \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create the model\n",
    "model = DenoisingDARTS()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming noisy_image is your corrupted image you want to restore\n",
    "noise_type = 'gaussian'\n",
    "resolution = 64\n",
    "noise_level = '0.09'\n",
    "epochs = 1000\n",
    "noisy_image = torch.tensor(np.load(f'/home/joe/nas-for-dip/phantoms/{noise_type}/res_{resolution}/nl_{noise_level}/p_{45}.npy')).float().to(device)\n",
    "input_noise = torch.randn(noisy_image.size()).to(noisy_image.device)  # Random noise matching the image size\n",
    "noisy_image = noisy_image.unsqueeze(1)  # Add channel dimension\n",
    "input_noise = input_noise.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "noisy_image = noisy_image.to(device)\n",
    "input_noise = input_noise.to(device)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    denoised_image = model(input_noise)  # Pass noise through the model\n",
    "    loss = criterion(denoised_image, noisy_image)\n",
    "    # compute PSNR\n",
    "    psnr = compare_psnr(noisy_image.cpu().detach().numpy(), denoised_image.cpu().detach().numpy())\n",
    "    print(f'Epoch {epoch} | Loss: {loss.item()} | PSNR: {psnr}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32, depth=6):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        features = init_features\n",
    "\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: torch.Size([1, 1, 64, 64])\n",
      "enc1: torch.Size([1, 32, 64, 64])\n",
      "pool1: torch.Size([1, 32, 32, 32])\n",
      "enc2: torch.Size([1, 64, 32, 32])\n",
      "pool2: torch.Size([1, 64, 16, 16])\n",
      "enc3: torch.Size([1, 128, 16, 16])\n",
      "pool3: torch.Size([1, 128, 8, 8])\n",
      "enc4: torch.Size([1, 256, 8, 8])\n",
      "pool4: torch.Size([1, 256, 4, 4])\n",
      "bottleneck: torch.Size([1, 512, 4, 4])\n",
      "\n",
      "\n",
      "upconv1: torch.Size([1, 256, 8, 8])\n",
      "cat1: torch.Size([1, 512, 8, 8])\n",
      "dec1: torch.Size([1, 256, 8, 8])\n",
      "upconv2: torch.Size([1, 128, 16, 16])\n",
      "cat2: torch.Size([1, 256, 16, 16])\n",
      "dec2: torch.Size([1, 128, 16, 16])\n",
      "upconv3: torch.Size([1, 64, 32, 32])\n",
      "cat3: torch.Size([1, 128, 32, 32])\n",
      "dec3: torch.Size([1, 64, 32, 32])\n",
      "upconv4: torch.Size([1, 32, 64, 64])\n",
      "cat4: torch.Size([1, 64, 64, 64])\n",
      "dec4: torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 1, 64, 64])\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=1, init_features=32, depth=4):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        features = init_features\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.encoders.append(UNet._block(in_channels, features, name=\"enc1\"))\n",
    "        self.pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        for i in range(depth-1):\n",
    "            self.encoders.append(UNet._block(features, features * 2, name=f\"enc{i+2}\"))\n",
    "            self.pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            features *= 2\n",
    "\n",
    "        self.bottleneck = UNet._block(features, features * 2, name=\"bottleneck\")\n",
    "\n",
    "        self.upconvs = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.upconvs.append(nn.ConvTranspose2d(\n",
    "                features * 2, features, kernel_size=2, stride=2\n",
    "            ))\n",
    "            self.decoders.append(UNet._block(features * 2, features, name=f\"dec{i+1}\"))\n",
    "            features //= 2\n",
    "        \n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features*2, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f'in: {x.shape}')\n",
    "        skips = []\n",
    "        for i in range(self.depth):\n",
    "            x = self.encoders[i](x)\n",
    "            print(f'enc{i+1}: {x.shape}')\n",
    "            skips.append(x)\n",
    "            x = self.pools[i](x)\n",
    "            print(f'pool{i+1}: {x.shape}')\n",
    "            \n",
    "        x = self.bottleneck(x)\n",
    "        print(f'bottleneck: {x.shape}\\n\\n')\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.upconvs[i](x)\n",
    "            print(f'upconv{i+1}: {x.shape}')\n",
    "            x = torch.cat((x, skips[-i-1]), dim=1)\n",
    "            print(f'cat{i+1}: {x.shape}')\n",
    "            x = self.decoders[i](x)\n",
    "            print(f'dec{i+1}: {x.shape}')\n",
    "        return torch.sigmoid(self.conv(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def test(self):\n",
    "        x = torch.randn(1, 1, 64, 64)\n",
    "        out = self.forward(x)\n",
    "        print(out.shape)\n",
    "        assert out.shape == (1, 1, 64, 64)\n",
    "        print('Test passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas-test-OHy8kATa-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
