{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from models import *\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.optim import Optimizer\n",
    "import time\n",
    "from utils.denoising_utils import *\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "TPU_IP_ADDRESS = '10.119.164.98'\n",
    "os.environ['TPU_IP_ADDRESS'] = TPU_IP_ADDRESS\n",
    "os.environ['XRT_TPU_CONFIG'] = f\"tpu_worker;0;{TPU_IP_ADDRESS}:8470\"\n",
    "\n",
    "# Initialize TPU device\n",
    "device = xm.xla_device()\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# display images\n",
    "def np_plot(np_matrix, title):\n",
    "    plt.clf()\n",
    "    fig = plt.imshow(np_matrix.transpose(1, 2, 0), interpolation = 'nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.05) \n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "print('TPU available: {}'.format(xm.xla_device()))\n",
    "\n",
    "fname = 'data/denoising/Dataset/image_Peppers512rgb.png'\n",
    "imsize =-1\n",
    "sigma = 25/255.\n",
    "img_pil = crop_image(get_image(fname, imsize)[0], d=32)\n",
    "img_np = pil_to_np(img_pil)                \n",
    "img_noisy_pil, img_noisy_np = get_noisy_image(img_np, sigma)\n",
    "np_plot(img_np, 'Natural image')\n",
    "np_plot(img_noisy_np, 'Noisy image')\n",
    "\n",
    "\n",
    "INPUT = 'noise'\n",
    "pad = 'reflection'\n",
    "OPT_OVER = 'net' # optimize over the net parameters only\n",
    "reg_noise_std = 1./30.\n",
    "learning_rate = LR = 0.01\n",
    "exp_weight=0.99\n",
    "input_depth = 32 \n",
    "roll_back = True # to prevent numerical issues\n",
    "burnin_iter = 7000 # burn-in iteration for SGLD\n",
    "weight_decay = 5e-8\n",
    "show_every =  500\n",
    "mse = torch.nn.MSELoss().type(dtype) # loss\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
    "img_torch = np_to_torch(img_np).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SGLD(Optimizer):\n",
    "    \"\"\" Stochastic Gradient Langevin Dynamics Sampler with preconditioning.\n",
    "        Optimization variable is viewed as a posterior sample under Stochastic\n",
    "        Gradient Langevin Dynamics with noise rescaled in eaach dimension\n",
    "        according to RMSProp.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-2,\n",
    "                 precondition_decay_rate=0.95,\n",
    "                 num_pseudo_batches=1,\n",
    "                 num_burn_in_steps=3000,\n",
    "                 diagonal_bias=1e-8) -> None:\n",
    "        \"\"\" Set up a SGLD Optimizer.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            params : iterable\n",
    "                Parameters serving as optimization variable.\n",
    "            lr : float, optional\n",
    "                Base learning rate for this optimizer.\n",
    "                Must be tuned to the specific function being minimized.\n",
    "                Default: `1e-2`.\n",
    "            precondition_decay_rate : float, optional\n",
    "                Exponential decay rate of the rescaling of the preconditioner (RMSprop).\n",
    "                Should be smaller than but nearly `1` to approximate sampling from the posterior.\n",
    "                Default: `0.95`\n",
    "            num_pseudo_batches : int, optional\n",
    "                Effective number of minibatches in the data set.\n",
    "                Trades off noise and prior with the SGD likelihood term.\n",
    "                Note: Assumes loss is taken as mean over a minibatch.\n",
    "                Otherwise, if the sum was taken, divide this number by the batch size.\n",
    "                Default: `1`.\n",
    "            num_burn_in_steps : int, optional\n",
    "                Number of iterations to collect gradient statistics to update the\n",
    "                preconditioner before starting to draw noisy samples.\n",
    "                Default: `3000`.\n",
    "            diagonal_bias : float, optional\n",
    "                Term added to the diagonal of the preconditioner to prevent it from\n",
    "                degenerating.\n",
    "                Default: `1e-8`.\n",
    "\n",
    "            \"\"\"\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if num_burn_in_steps < 0:\n",
    "            raise ValueError(\"Invalid num_burn_in_steps: {}\".format(num_burn_in_steps))\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr, precondition_decay_rate=precondition_decay_rate,\n",
    "            num_pseudo_batches=num_pseudo_batches,\n",
    "            num_burn_in_steps=num_burn_in_steps,\n",
    "            diagonal_bias=1e-8,\n",
    "        )\n",
    "        # Use XLA device for TPU support\n",
    "        self.device = xm.xla_device()\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "\n",
    "        # Inside your step method\n",
    "        for group in self.param_groups:\n",
    "            for parameter in group[\"params\"]:\n",
    "                if parameter.grad is None:\n",
    "                    continue\n",
    "\n",
    "                # The state can remain on the CPU\n",
    "                state = self.state[parameter]\n",
    "\n",
    "                # Move parameters and grads to TPU\n",
    "                parameter = parameter.to(self.device)\n",
    "                gradient = parameter.grad.data.to(self.device)\n",
    "\n",
    "                lr = group[\"lr\"]\n",
    "                num_pseudo_batches = group[\"num_pseudo_batches\"]\n",
    "                precondition_decay_rate = group[\"precondition_decay_rate\"]\n",
    "\n",
    "                # Initialize state if it's empty\n",
    "                if len(state) == 0:\n",
    "                    state[\"iteration\"] = 0\n",
    "                    state[\"momentum\"] = torch.ones_like(parameter).to(self.device)\n",
    "                \n",
    "                state[\"iteration\"] += 1\n",
    "\n",
    "                # Move momentum to TPU\n",
    "                momentum = state[\"momentum\"].to(self.device)\n",
    "\n",
    "                #  Momentum update {{{ #\n",
    "                momentum.add_(\n",
    "                    (1.0 - precondition_decay_rate) * ((gradient ** 2) - momentum)\n",
    "                )\n",
    "                #  }}} Momentum update #\n",
    "\n",
    "                if state[\"iteration\"] > group[\"num_burn_in_steps\"]:\n",
    "                    sigma = 1. / torch.sqrt(torch.tensor(lr))\n",
    "                else:\n",
    "                    sigma = torch.zeros_like(parameter)\n",
    "\n",
    "                preconditioner = (\n",
    "                    1. / torch.sqrt(momentum + group[\"diagonal_bias\"])\n",
    "                )\n",
    "\n",
    "                scaled_grad = (\n",
    "                    0.5 * preconditioner * gradient * num_pseudo_batches +\n",
    "                    torch.normal(\n",
    "                        mean=torch.zeros_like(gradient),\n",
    "                        std=torch.ones_like(gradient)\n",
    "                    ) * sigma * torch.sqrt(preconditioner)\n",
    "                )\n",
    "\n",
    "                parameter.data.add_(-lr * scaled_grad)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgld_psnr_list = [] # psnr between sgld out and gt\n",
    "sgld_mean = 0\n",
    "roll_back = True # To solve the oscillation of model training \n",
    "last_net = None\n",
    "psrn_noisy_last = 0\n",
    "MCMC_iter = 50\n",
    "param_noise_sigma = 2\n",
    "\n",
    "sgld_mean_each = 0\n",
    "sgld_psnr_mean_list = [] # record the PSNR of avg after burn-in\n",
    "\n",
    "sample_count = 0  # Initialize your sample_count\n",
    "sgld_mean = 0  # Initialize your sgld_mean\n",
    "\n",
    "i = 0\n",
    "i = torch.tensor(i, dtype=torch.int)\n",
    "burnin_iter = torch.tensor(burnin_iter, dtype=torch.int)\n",
    "MCMC_iter = torch.tensor(MCMC_iter, dtype=torch.int)\n",
    "sgld_mean = torch.tensor(sgld_mean, dtype=torch.float32)\n",
    "sample_count = torch.tensor(sample_count, dtype=torch.float32)\n",
    "\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "net_input = get_noise(input_depth, INPUT, (img_pil.size[1], img_pil.size[0])).type(dtype).detach().to(device)\n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "\n",
    "# Move other tensors to device if needed\n",
    "img_noisy_torch = img_noisy_torch.to(device)\n",
    "\n",
    "sample_count = 0\n",
    "\n",
    "def closure_sgld():\n",
    "    global i, net_input, sgld_mean, sample_count, psrn_noisy_last, last_net, sgld_mean_each\n",
    "    if reg_noise_std > 0: # reg_noise_std is always 1/30\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "\n",
    "    out = model(net_input)\n",
    "    total_loss = mse(out, img_noisy_torch)\n",
    "    total_loss.backward()\n",
    "\n",
    "\n",
    "    # psrn_noisy = psnr_tensor(img_noisy_torch, out)\n",
    "    # psrn_gt    = psnr_tensor(img_torch, out)\n",
    "    \n",
    "    # if i > burnin_iter and (i % MCMC_iter == 0):\n",
    "    #     sgld_mean += out\n",
    "    #     sample_count += 1.\n",
    "\n",
    "\n",
    "    # if i > burnin_iter:\n",
    "    #     sgld_mean_each += out\n",
    "    #     sgld_mean_tmp = sgld_mean_each / (i - burnin_iter)\n",
    "    #     sgld_mean_psnr_each = psnr_tensor(img_torch, sgld_mean_tmp)\n",
    "    #     print('Iter: %d; psnr_gt %.2f; psnr_sgld %.2f' % (i, psrn_gt, sgld_mean_psnr_each))\n",
    "    # else:\n",
    "    #     print('Iter: %d; psnr_gt %.2f; loss %.5f' % (i, psrn_gt, total_loss))\n",
    "    \n",
    "    # if i == burnin_iter:\n",
    "    #     print('Burn-in done, start sampling')\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def psnr_tensor(img1, img2, max_val=1.0):\n",
    "    \"\"\"Compute Peak Signal-to-Noise Ratio (the higher the better).\n",
    "\n",
    "    img1 and img2 have range [0, max_val]\n",
    "    \"\"\"\n",
    "    img1 = img1.to(torch.float32)\n",
    "    img2 = img2.to(torch.float32)\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 10 * torch.log10(max_val ** 2 / mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=32, out_channels=3, init_features=128, pretrained=False)\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SGLD optimizer\n",
    "print('Starting optimization with SGLD')\n",
    "num_iter = 500 # max iterations\n",
    "\n",
    "optimizer = SGLD(\n",
    "    model.parameters(), \n",
    "    lr=LR,\n",
    "    precondition_decay_rate=1-5e-8,\n",
    "    num_burn_in_steps=7000\n",
    "    )  # SGLD hyperparameters here, if any\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=weight_decay)\n",
    "\n",
    "# Replace Adam optimizer with SGLD in the training loop\n",
    "for j in range(num_iter):\n",
    "    # print('Iteration %05d/%05d' % (j + 1, num_iter))\n",
    "    adam.zero_grad()\n",
    "    \n",
    "    # loss = closure_sgld()  # Assuming closure_sgld() computes the loss and performs backward\n",
    "    \n",
    "\n",
    "    out = model(net_input)\n",
    "    total_loss = nn.MSELoss()(out, img_noisy_torch)\n",
    "    total_loss.backward()\n",
    "\n",
    "    # net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "    # out = model(net_input)\n",
    "    # total_loss = mse(out, img_noisy_torch)\n",
    "    # print('Iter: %d; loss %.5f' % (j, total_loss))\n",
    "    # total_loss.backward()\n",
    "\n",
    "    adam.step()\n",
    "    xm.mark_step()\n",
    "    \n",
    "sgld_mean = sgld_mean / sample_count  # Finalize your sgld_mean\n",
    "sgld_mean_psnr = psnr_tensor(img_np, sgld_mean)  # Compute PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the shape of img_np\n",
    "print('img_np shape: %f' % img_np.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
