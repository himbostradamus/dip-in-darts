{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "GPUs = GPUtil.getGPUs()\n",
    "for gpu in GPUs:\n",
    "  print(gpu.name, gpu.memoryTotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_eval.eval_generic import SGLDES\n",
    "from search_eval.optimizer.SingleImageDataset import SingleImageDataset\n",
    "from search_eval.utils.common_utils import *\n",
    "from search_space.node_space import NodeSpace\n",
    "from search_space.unet.unetspaceMT import UNetSpaceMT\n",
    "\n",
    "from nni import trace\n",
    "import nni.retiarii.strategy as strategy\n",
    "import nni.retiarii.serializer as serializer\n",
    "\n",
    "from nni.retiarii.experiment.pytorch import RetiariiExperiment, RetiariiExeConfig\n",
    "from nni.retiarii.evaluator.pytorch import Lightning, Trainer\n",
    "from nni.retiarii.evaluator.pytorch.lightning import DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "import nni.retiarii.nn.pytorch as nn\n",
    "from nni.retiarii import model_wrapper\n",
    "from nni.retiarii.nn.pytorch import Cell\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@trace\n",
    "def conv_2d(C_in, C_out, kernel_size=3, dilation=1, padding=1, activation=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(C_in, C_out, kernel_size=kernel_size, dilation=dilation, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation,\n",
    "        nn.Conv2d(C_out, C_out, kernel_size=kernel_size, dilation=dilation, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation\n",
    "    )\n",
    "\n",
    "@trace\n",
    "def depthwise_separable_conv(C_in, C_out, kernel_size=3, dilation=1, padding=1, activation=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(C_in, C_in, kernel_size=kernel_size, dilation=dilation, padding=padding, groups=C_in, bias=False),\n",
    "        nn.Conv2d(C_in, C_out, 1, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation,\n",
    "        nn.Conv2d(C_out, C_out, kernel_size=kernel_size, dilation=dilation, padding=padding, groups=C_out, bias=False),\n",
    "        nn.Conv2d(C_out, C_out, 1, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation\n",
    "    )\n",
    "\n",
    "@trace\n",
    "def transposed_conv_2d(C_in, C_out, kernel_size=4, stride=2, padding=1, activation=None):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(C_in, C_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation\n",
    "    )\n",
    "\n",
    "def pools():\n",
    "    pool_dict = OrderedDict([\n",
    "        (\"MaxPool2d\", nn.MaxPool2d(kernel_size=2, stride=2, padding=0)),\n",
    "        (\"AvgPool2d\", nn.AvgPool2d(kernel_size=2, stride=2, padding=0)),\n",
    "        # (\"AdaMaxPool2d\", nn.AdaptiveMaxPool2d(1)),\n",
    "        # (\"AdaAvgPool2d\", nn.AdaptiveAvgPool2d(1)),\n",
    "        # (\"DepthToSpace\", nn.PixelShuffle(2)),\n",
    "    ])\n",
    "    return pool_dict\n",
    "\n",
    "def upsamples(C_in, C_out):\n",
    "    upsample_dict = OrderedDict([\n",
    "        (\"Upsample_nearest\", nn.Upsample(scale_factor=2, mode='nearest')),\n",
    "        (\"Upsample_bilinear\", nn.Upsample(scale_factor=2, mode='bilinear')),\n",
    "        (\"TransConv_4x4_Relu\", transposed_conv_2d(C_in, C_out)),\n",
    "        (\"TransConv_2x2_RelU\", transposed_conv_2d(C_in, C_out, kernel_size=2, stride=2, padding=0)),\n",
    "    ])\n",
    "    return upsample_dict\n",
    "\n",
    "def convs(C_in, C_out):\n",
    "    # all padding should follow this formula:\n",
    "    # pd = (ks - 1) * dl // 2\n",
    "    conv_dict = OrderedDict([\n",
    "        \n",
    "        (\"conv2d_1x1_Relu\", conv_2d(C_in, C_out)),\n",
    "        # (\"conv2d_1x1_SiLU\", conv_2d(C_in, C_out, activation=nn.SiLU())),\n",
    "\n",
    "        (\"conv2d_3x3_Relu\", conv_2d(C_in, C_out, kernel_size=3, padding=1)),\n",
    "        (\"conv2d_3x3_SiLU\", conv_2d(C_in, C_out, kernel_size=3, padding=1, activation=nn.SiLU())),\n",
    "        (\"conv2d_3x3_Sigmoid\", conv_2d(C_in, C_out, kernel_size=3, padding=1, activation=nn.Sigmoid())),\n",
    "        # (\"conv2d_3x3_Relu_1dil\", conv_2d(C_in, C_out, kernel_size=3, padding=2, dilation=2)),\n",
    "\n",
    "        # (\"conv2d_5x5_Relu\", conv_2d(C_in, C_out, kernel_size=5, padding=2)),\n",
    "        # (\"conv2d_5x5_Relu_1dil\", conv_2d(C_in, C_out, kernel_size=5, padding=4, dilation=2, activation=nn.SiLU())),\n",
    "        # (\"conv2d_5x5_SiLU\", conv_2d(C_in, C_out, kernel_size=5, padding=2, activation=nn.SiLU())),\n",
    "\n",
    "\n",
    "        # (\"convDS_1x1_Relu\", depthwise_separable_conv(C_in, C_out)),\n",
    "        (\"convDS_1x1_SiLU\", depthwise_separable_conv(C_in, C_out, activation=nn.SiLU())),\n",
    "\n",
    "        (\"convDS_3x3_Relu\", depthwise_separable_conv(C_in, C_out, kernel_size=3, padding=1)),\n",
    "        (\"convDS_3x3_SiLU\", depthwise_separable_conv(C_in, C_out, kernel_size=3, padding=1, activation=nn.SiLU())),\n",
    "\n",
    "        # (\"convDS_5x5_Relu\", depthwise_separable_conv(C_in, C_out, kernel_size=5, padding=2)),\n",
    "        # (\"convDS_5x5_SiLU\", depthwise_separable_conv(C_in, C_out, kernel_size=5, padding=2, activation=nn.SiLU())),\n",
    "    ])\n",
    "    return conv_dict\n",
    "\n",
    "@trace\n",
    "@model_wrapper\n",
    "class SearchSpace(nn.Module):\n",
    "    def __init__(self, C_in=1, C_out=1, depth=4, enNodes=1, deNodes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all padding should follow this formula:\n",
    "        # pd = (ks - 1) * dl // 2\n",
    "        self.pr = False\n",
    "        self.depth = depth\n",
    "        \n",
    "        self.in_layer = nn.Conv2d(C_in, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Encoders\n",
    "        filters = 64\n",
    "        self.encoders = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            enNodes = enNodes\n",
    "            pool_candidates = pools()\n",
    "            enConv_candidates = convs(filters, filters*2 // enNodes)\n",
    "\n",
    "            self.encoders.append(Cell(\n",
    "                op_candidates=pool_candidates, \n",
    "                num_nodes=1, \n",
    "                num_ops_per_node=1, #len(pool_candidates), \n",
    "                num_predecessors=1, \n",
    "                label=f'pool_{i+1}',\n",
    "                ))\n",
    "            self.encoders.append(Cell(\n",
    "                op_candidates=enConv_candidates, \n",
    "                num_nodes=enNodes, \n",
    "                num_ops_per_node=1,\n",
    "                num_predecessors=1, \n",
    "                label=f'conv_{i+1}',\n",
    "                ))\n",
    "            filters *= 2\n",
    "\n",
    "        # Decoders\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            deNodes = deNodes\n",
    "            upsample_candidates = upsamples(filters, filters)\n",
    "\n",
    "            self.decoders.append(Cell(\n",
    "                op_candidates=upsample_candidates, \n",
    "                num_nodes=1, \n",
    "                num_ops_per_node=1, #len(upsample_candidates), \n",
    "                num_predecessors=1, \n",
    "                label=f'upsample_{i+1}'))\n",
    "            \n",
    "            filters //= 2\n",
    "\n",
    "            deConv_candidates = convs(filters*3, filters // deNodes)\n",
    "\n",
    "            self.decoders.append(Cell(\n",
    "                op_candidates=deConv_candidates, \n",
    "                num_nodes=deNodes, \n",
    "                num_ops_per_node=1, #len(deConv_candidates), \n",
    "                num_predecessors=1, \n",
    "                label=f'conv_{i+1+depth}'))\n",
    "\n",
    "        self.out_layer = nn.Conv2d(64, C_out, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.pr:\n",
    "            print(f'input shape: {x.shape}\\n')\n",
    "\n",
    "        # print(f'input shape: {x.shape}')\n",
    "        x = self.in_layer(x)  # Apply the initial layer\n",
    "        # print(f'post in_layer shape: {x.shape}\\n')\n",
    "        skip_connections = [x]\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.encoders[2*i]([x])\n",
    "            x = self.encoders[2*i+1]([x])\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            upsampled = self.decoders[2*i]([x])\n",
    "            cropped = self.crop_tensor(upsampled, skip_connections[-(i+2)])\n",
    "            x = torch.cat([cropped, upsampled], 1)\n",
    "            x = self.decoders[2*i+1]([x])\n",
    "\n",
    "        x = self.out_layer(x)  # Apply the final layer\n",
    "\n",
    "        return x\n",
    "\n",
    "    def crop_tensor(self, target_tensor, tensor):\n",
    "        target_size = target_tensor.size()[2]  # Assuming height and width are same\n",
    "        tensor_size = tensor.size()[2]\n",
    "        delta = tensor_size - target_size\n",
    "        delta = delta // 2\n",
    "        return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        This will input a random tensor of 1x1x128x128 and test the forward pass.\n",
    "        \"\"\"\n",
    "        self.pr = True\n",
    "        x = torch.randn(1, 1, 128, 128)\n",
    "        y = self.forward(x)\n",
    "        assert y.shape == (1, 1, 128, 128), \"Output shape should be (1, 1, 128, 128), got {}\".format(y.shape)\n",
    "        print(f'output shape: {y.shape}\\n')\n",
    "        print(\"Test passed.\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def get_U_Net(in_channels=1, out_channels=1, init_features=64, pretrained=False):\n",
    "    return torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=in_channels, out_channels=out_channels, init_features=init_features, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_strategy = strategy.DARTS()\n",
    "\n",
    "\n",
    "\n",
    "@trace\n",
    "def conv_2d(C_in, C_out, kernel_size=3, dilation=1, padding=1, activation=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(C_in, C_out, kernel_size=kernel_size, dilation=dilation, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation,\n",
    "        nn.Conv2d(C_out, C_out, kernel_size=kernel_size, dilation=dilation, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation\n",
    "    )\n",
    "\n",
    "@trace\n",
    "def depthwise_separable_conv(C_in, C_out, kernel_size=3, dilation=1, padding=1, activation=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(C_in, C_in, kernel_size=kernel_size, dilation=dilation, padding=padding, groups=C_in, bias=False),\n",
    "        nn.Conv2d(C_in, C_out, 1, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation,\n",
    "        nn.Conv2d(C_out, C_out, kernel_size=kernel_size, dilation=dilation, padding=padding, groups=C_out, bias=False),\n",
    "        nn.Conv2d(C_out, C_out, 1, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation\n",
    "    )\n",
    "\n",
    "@trace\n",
    "def transposed_conv_2d(C_in, C_out, kernel_size=4, stride=2, padding=1, activation=None):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(C_in, C_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(C_out),\n",
    "        nn.ReLU() if activation is None else activation\n",
    "    )\n",
    "\n",
    "def pools():\n",
    "    pool_dict = OrderedDict([\n",
    "        (\"MaxPool2d\", nn.MaxPool2d(kernel_size=2, stride=2, padding=0)),\n",
    "        (\"AvgPool2d\", nn.AvgPool2d(kernel_size=2, stride=2, padding=0)),\n",
    "        # (\"AdaMaxPool2d\", nn.AdaptiveMaxPool2d(1)),\n",
    "        # (\"AdaAvgPool2d\", nn.AdaptiveAvgPool2d(1)),\n",
    "        # (\"DepthToSpace\", nn.PixelShuffle(2)),\n",
    "    ])\n",
    "    return pool_dict\n",
    "\n",
    "def upsamples(C_in, C_out):\n",
    "    upsample_dict = OrderedDict([\n",
    "        (\"Upsample_nearest\", nn.Upsample(scale_factor=2, mode='nearest')),\n",
    "        (\"Upsample_bilinear\", nn.Upsample(scale_factor=2, mode='bilinear')),\n",
    "        (\"TransConv_4x4_Relu\", transposed_conv_2d(C_in, C_out)),\n",
    "        (\"TransConv_2x2_RelU\", transposed_conv_2d(C_in, C_out, kernel_size=2, stride=2, padding=0)),\n",
    "    ])\n",
    "    return upsample_dict\n",
    "\n",
    "def convs(C_in, C_out):\n",
    "    # all padding should follow this formula:\n",
    "    # pd = (ks - 1) * dl // 2\n",
    "    conv_dict = OrderedDict([\n",
    "        \n",
    "        (\"conv2d_1x1_Relu\", conv_2d(C_in, C_out)),\n",
    "        # (\"conv2d_1x1_SiLU\", conv_2d(C_in, C_out, activation=nn.SiLU())),\n",
    "\n",
    "        (\"conv2d_3x3_Relu\", conv_2d(C_in, C_out, kernel_size=3, padding=1)),\n",
    "        (\"conv2d_3x3_SiLU\", conv_2d(C_in, C_out, kernel_size=3, padding=1, activation=nn.SiLU())),\n",
    "        (\"conv2d_3x3_Sigmoid\", conv_2d(C_in, C_out, kernel_size=3, padding=1, activation=nn.Sigmoid())),\n",
    "        # (\"conv2d_3x3_Relu_1dil\", conv_2d(C_in, C_out, kernel_size=3, padding=2, dilation=2)),\n",
    "\n",
    "        # (\"conv2d_5x5_Relu\", conv_2d(C_in, C_out, kernel_size=5, padding=2)),\n",
    "        # (\"conv2d_5x5_Relu_1dil\", conv_2d(C_in, C_out, kernel_size=5, padding=4, dilation=2, activation=nn.SiLU())),\n",
    "        # (\"conv2d_5x5_SiLU\", conv_2d(C_in, C_out, kernel_size=5, padding=2, activation=nn.SiLU())),\n",
    "\n",
    "\n",
    "        # (\"convDS_1x1_Relu\", depthwise_separable_conv(C_in, C_out)),\n",
    "        (\"convDS_1x1_SiLU\", depthwise_separable_conv(C_in, C_out, activation=nn.SiLU())),\n",
    "\n",
    "        (\"convDS_3x3_Relu\", depthwise_separable_conv(C_in, C_out, kernel_size=3, padding=1)),\n",
    "        (\"convDS_3x3_SiLU\", depthwise_separable_conv(C_in, C_out, kernel_size=3, padding=1, activation=nn.SiLU())),\n",
    "\n",
    "        # (\"convDS_5x5_Relu\", depthwise_separable_conv(C_in, C_out, kernel_size=5, padding=2)),\n",
    "        # (\"convDS_5x5_SiLU\", depthwise_separable_conv(C_in, C_out, kernel_size=5, padding=2, activation=nn.SiLU())),\n",
    "    ])\n",
    "    return conv_dict\n",
    "\n",
    "# Encoder preprocessor \n",
    "class EncoderPreprocessor(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return [self.conv(x) for x in inputs]\n",
    "\n",
    "@trace\n",
    "@model_wrapper\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            C_in=1, \n",
    "            C_out=1, \n",
    "            depth=2, \n",
    "            nodes_per_layer=1,\n",
    "            ops_per_node=1,\n",
    "            poolOps_per_node=1,\n",
    "            upsampleOps_per_node=1,\n",
    "            \n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.nodes = nodes_per_layer\n",
    "        \n",
    "        nodes = nodes_per_layer\n",
    "        start_filters = end_filters = 64\n",
    "        self.in_layer = nn.Conv2d(C_in, start_filters, kernel_size=3, padding=1)\n",
    "\n",
    "        # encoder layers\n",
    "        mid_in = 64\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.postencoders = nn.ModuleList()\n",
    "        for _ in range(self.depth):\n",
    "            self.pools.append(Cell(\n",
    "                op_candidates=pools(),\n",
    "                num_nodes=1, \n",
    "                num_ops_per_node=poolOps_per_node,\n",
    "                num_predecessors=1, \n",
    "            ))\n",
    "            self.encoders.append(Cell(\n",
    "                op_candidates=convs(mid_in,mid_in),\n",
    "                num_nodes=nodes, \n",
    "                num_ops_per_node=ops_per_node,\n",
    "                num_predecessors=1, \n",
    "            ))\n",
    "            self.postencoders.append(nn.Conv2d(mid_in*nodes, mid_in*2, kernel_size=3, padding=1))\n",
    "            mid_in *= 2\n",
    "\n",
    "        # decoder layers\n",
    "        self.upsamples = nn.ModuleList()\n",
    "        self.predecoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.postdecoders = nn.ModuleList()\n",
    "        for _ in range(self.depth):\n",
    "            self.upsamples.append(Cell(\n",
    "                op_candidates=upsamples(mid_in,mid_in),\n",
    "                num_nodes=1, \n",
    "                num_ops_per_node=upsampleOps_per_node,\n",
    "                num_predecessors=1, \n",
    "            ))\n",
    "            mid_in //= 2\n",
    "            self.predecoders.append(nn.Conv2d(mid_in*3, mid_in, kernel_size=3, padding=1))\n",
    "            self.decoders.append(Cell(\n",
    "                op_candidates=convs(mid_in,mid_in),\n",
    "                num_nodes=nodes, \n",
    "                num_ops_per_node=ops_per_node,\n",
    "                num_predecessors=1, \n",
    "            ))\n",
    "            self.postdecoders.append(nn.Conv2d(mid_in*nodes, mid_in, kernel_size=3, padding=1))\n",
    "\n",
    "        self.out_layer = nn.Conv2d(end_filters, C_out, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.in_layer(x)\n",
    "\n",
    "        skip_connections = [x]\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.pools[i]([x])\n",
    "            x = self.encoders[i]([x])\n",
    "            x = self.postencoders[i](x)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            upsampled = self.upsamples[i]([x])\n",
    "            cropped = self.crop_tensor(upsampled, skip_connections[-(i+2)])\n",
    "            x = torch.cat([cropped, upsampled], 1)\n",
    "            x = self.predecoders[i](x)\n",
    "            x = self.decoders[i]([x])\n",
    "            x = self.postdecoders[i](x)\n",
    "\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "    def crop_tensor(self, target_tensor, tensor):\n",
    "        target_size = target_tensor.size()[2]  # Assuming height and width are same\n",
    "        tensor_size = tensor.size()[2]\n",
    "        delta = tensor_size - target_size\n",
    "        delta = delta // 2\n",
    "        return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m noise_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgaussian\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m noise_level \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0.09\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m phantom \u001b[39m=\u001b[39m       np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/joe/nas-for-dip/phantoms/ground_truth/\u001b[39m\u001b[39m{\u001b[39;00mresolution\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m45\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m phantom_noisy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/joe/nas-for-dip/phantoms/\u001b[39m\u001b[39m{\u001b[39;00mnoise_type\u001b[39m}\u001b[39;00m\u001b[39m/res_\u001b[39m\u001b[39m{\u001b[39;00mresolution\u001b[39m}\u001b[39;00m\u001b[39m/nl_\u001b[39m\u001b[39m{\u001b[39;00mnoise_level\u001b[39m}\u001b[39;00m\u001b[39m/p_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m45\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Create the lightning module\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "total_iterations = 1200 # 650 for nodespace\n",
    "\n",
    "resolution = 64\n",
    "noise_type = 'gaussian'\n",
    "noise_level = '0.09'\n",
    "phantom =       np.load(f'/home/joe/nas-for-dip/phantoms/ground_truth/{resolution}/{45}.npy')\n",
    "phantom_noisy = np.load(f'/home/joe/nas-for-dip/phantoms/{noise_type}/res_{resolution}/nl_{noise_level}/p_{45}.npy')\n",
    "\n",
    "# Create the lightning module\n",
    "learning_rate = 0.1\n",
    "buffer_size = 100\n",
    "patience = 500 # 75 for NodeSpace\n",
    "weight_decay = 5e-5\n",
    "show_every = 200\n",
    "report_every = 50\n",
    "\n",
    "module = SGLDES(\n",
    "                phantom=phantom, \n",
    "                phantom_noisy=phantom_noisy,\n",
    "                \n",
    "                learning_rate=learning_rate, \n",
    "                buffer_size=buffer_size,\n",
    "                patience=patience,\n",
    "                weight_decay= weight_decay,\n",
    "\n",
    "                show_every=show_every,\n",
    "                report_every=report_every,\n",
    "                HPO=False,\n",
    "                NAS=True,\n",
    "                OneShot=False,\n",
    "                SGLD_regularize=True,\n",
    "                switch=None,\n",
    "                MCMC_iter=10\n",
    "                )\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = Trainer(\n",
    "            max_epochs=total_iterations,\n",
    "            fast_dev_run=False,\n",
    "            gpus=1,\n",
    "            )\n",
    "            \n",
    "if not hasattr(trainer, 'optimizer_frequencies'):\n",
    "    trainer.optimizer_frequencies = []\n",
    "\n",
    "\n",
    "# Create the lighting object for evaluator\n",
    "train_loader = DataLoader(SingleImageDataset(phantom, num_iter=1), batch_size=1)\n",
    "val_loader = DataLoader(SingleImageDataset(phantom, num_iter=1), batch_size=1)\n",
    "\n",
    "lightning = Lightning(lightning_module=module, trainer=trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "\n",
    "# Create a Search Space\n",
    "depth = 4\n",
    "nodes_per_layer = 1\n",
    "ops_per_node = 1\n",
    "poolOps_per_node = 1\n",
    "upsampleOps_per_node = 1\n",
    "# model_space = NodeSpace(\n",
    "#          depth=depth, \n",
    "#          nodes_per_layer=nodes_per_layer,\n",
    "#          ops_per_node=ops_per_node, \n",
    "#          poolOps_per_node=poolOps_per_node, \n",
    "#          upsampleOps_per_node=upsampleOps_per_node\n",
    "#         )\n",
    "\n",
    "model_space = UNetSpaceMT()\n",
    "\n",
    "# Select the Search Strategy\n",
    "# search_strategy = strategy.DARTS()\n",
    "# # search_strategy = strategy.ENAS()\n",
    "# search_strategy = strategy.GumbelDARTS()\n",
    "# # search_strategy = strategy.RandomOneShot()\n",
    "\n",
    "# Select a Search Strategy\n",
    "search_strategy = strategy.Random(dedup=True)\n",
    "# search_strategy = strategy.TPE()\n",
    "# search_strategy = strategy.RegularizedEvolution(dedup=True)\n",
    "\n",
    "# fast_dev_run=False\n",
    "print(f'\\n\\n----------------------------------\\n')\n",
    "print(f'Configration:\\n')\n",
    "print(f'total_iterations: {total_iterations}')\n",
    "print(f'resolution: {resolution}')\n",
    "print(f'noise_type: {noise_type}')\n",
    "print(f'noise_level: {noise_level}')\n",
    "\n",
    "print(f'\\n-------++++++++++++++++++++---------\\n')\n",
    "\n",
    "print(f'depth: {depth}')\n",
    "print(f'nodes_per_layer: {nodes_per_layer}')\n",
    "print(f'ops_per_node: {ops_per_node}')\n",
    "# print(f'poolOps_per_node: {poolOps_per_node}')\n",
    "# print(f'upsampleOps_per_node: {upsampleOps_per_node}')\n",
    "\n",
    "print(f'\\n-------++++++++++++++++++++---------\\n')\n",
    "print(f'strategy: {strategy.__class__.__name__}')\n",
    "\n",
    "print(f'\\n----------------------------------\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "config = RetiariiExeConfig(execution_engine='oneshot')\n",
    "experiment = RetiariiExperiment(model_space, evaluator=lightning, strategy=search_strategy)\n",
    "experiment.run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain top model\n",
    "from search_space.node_space import exportedModel\n",
    "from search_eval.eval_no_search_SGLD_ES import Eval_SGLD_ES\n",
    "\n",
    "# construct output and retrain\n",
    "exported_arch = experiment.export_top_models()[0]\n",
    "# extract value from key -- pool 0\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "for i in range(depth):\n",
    "    print(f'pool {i+1}: ', exported_arch[f'pool {i}/op_1_0'])\n",
    "    print(f'encoder {i+1}: ', exported_arch[f'encoder {i}/op_1_0'])\n",
    "    print(\"--------------------\")\n",
    "for i in range(depth):\n",
    "    print(f'upsample {i+1}: ', exported_arch[f'upsample {i}/op_1_0'])\n",
    "    print(f'decoder {i+1}: ', exported_arch[f'decoder {i}/op_1_0'])\n",
    "    print(\"--------------------\")\n",
    "print(\"--------------------\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop experiment and clear cache\n",
    "experiment.stop()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 1, 64, 64])\n",
      "after in layer: torch.Size([1, 64, 64, 64])\n",
      "after pool 0: torch.Size([1, 64, 32, 32])\n",
      "after enc 0: torch.Size([1, 128, 32, 32])\n",
      "\n",
      "after attention 0: torch.Size([1, 128, 32, 32])\n",
      "\n",
      "\n",
      "after pool 1: torch.Size([1, 128, 16, 16])\n",
      "after enc 1: torch.Size([1, 256, 16, 16])\n",
      "\n",
      "after pool 2: torch.Size([1, 256, 8, 8])\n",
      "after enc 2: torch.Size([1, 512, 8, 8])\n",
      "\n",
      "after attention 2: torch.Size([1, 512, 8, 8])\n",
      "\n",
      "\n",
      "after pool 3: torch.Size([1, 512, 4, 4])\n",
      "after enc 3: torch.Size([1, 1024, 4, 4])\n",
      "\n",
      "after upsample 0: torch.Size([1, 1024, 6, 6])\n",
      "after dec 0: torch.Size([1, 512, 6, 6])\n",
      "\n",
      "after attention 0: torch.Size([1, 512, 6, 6])\n",
      "\n",
      "\n",
      "after upsample 1: torch.Size([1, 512, 10, 10])\n",
      "after dec 1: torch.Size([1, 256, 10, 10])\n",
      "\n",
      "after upsample 2: torch.Size([1, 256, 18, 18])\n",
      "after dec 2: torch.Size([1, 128, 18, 18])\n",
      "\n",
      "after attention 2: torch.Size([1, 128, 18, 18])\n",
      "\n",
      "\n",
      "after upsample 3: torch.Size([1, 128, 34, 34])\n",
      "after dec 3: torch.Size([1, 64, 34, 34])\n",
      "\n",
      "torch.Size([1, 1, 34, 34])\n"
     ]
    }
   ],
   "source": [
    "from search_space.unetAttention import UNetWithAttention\n",
    "model = UNetWithAttention()\n",
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas-test-OHy8kATa-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
