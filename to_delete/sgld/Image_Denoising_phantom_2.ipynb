{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from models import *\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch import tensor\n",
    "import time\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from utils.denoising_utils import *\n",
    "\n",
    "import seaborn as sns\n",
    "from phantom import generate_phantom\n",
    "\n",
    "from nni.retiarii.evaluator.pytorch import Lightning, Trainer, LightningModule\n",
    "from nni.retiarii.evaluator.pytorch.lightning import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# display images\n",
    "def np_plot(np_matrix, title, cmap=None):\n",
    "    plt.clf()\n",
    "    if cmap is not None:\n",
    "        fig = plt.imshow(np_matrix.transpose(1, 2, 0), interpolation = 'nearest',cmap='gray')\n",
    "    else:\n",
    "        fig = plt.imshow(np_matrix.transpose(1, 2, 0), interpolation = 'nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.05) \n",
    "\n",
    "\n",
    "def get_unet():\n",
    "    return torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/denoising/Dataset/image_Peppers512rgb.png'\n",
    "imsize =-1\n",
    "sigma = 25/255.\n",
    "img_pil = crop_image(get_image(fname, imsize)[0], d=32)\n",
    "img_np = pil_to_np(img_pil)                \n",
    "img_noisy_pil, img_noisy_np = get_noisy_image(img_np, sigma)\n",
    "np_plot(img_np, 'Natural image')\n",
    "np_plot(img_noisy_np, 'Noisy image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT = 'noise'\n",
    "pad = 'reflection'\n",
    "OPT_OVER = 'net' # optimize over the net parameters only\n",
    "reg_noise_std = 1./30.\n",
    "learning_rate = LR = 0.01\n",
    "exp_weight=0.99\n",
    "input_depth = 32 \n",
    "roll_back = True # to prevent numerical issues\n",
    "num_iter = 20000 # max iterations\n",
    "burnin_iter = 7000 # burn-in iteration for SGLD\n",
    "weight_decay = 5e-8\n",
    "show_every =  500\n",
    "mse = torch.nn.MSELoss().type(dtype) # loss\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phantom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGLD Pytorch Lightning Module\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, image, num_iter):\n",
    "        self.image = image\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_iter\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Always return the same image (and maybe a noise tensor or other information if necessary??)\n",
    "        return self.image\n",
    "\n",
    "class SGLD(LightningModule):\n",
    "    def __init__(self, \n",
    "        original_np,\n",
    "        noisy_np,\n",
    "        noisy_torch,\n",
    "        learning_rate = 0.01,\n",
    "        show_every=20,\n",
    "        patience = 1000,\n",
    "        buffer_size = 100,\n",
    "        model=get_unet()\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # iterators\n",
    "        self.burnin_iter=0 # burn-in iteration for SGLD\n",
    "        self.show_every=show_every\n",
    "        self.num_iter=20000\n",
    "\n",
    "        # backtracking\n",
    "        self.psrn_noisy_last=0\n",
    "        self.last_net = None\n",
    "        self.roll_back = True # To solve the oscillation of model training \n",
    "\n",
    "        # SGLD Output Accumulation\n",
    "        self.sgld_mean=0\n",
    "        self.sgld_mean_each=0\n",
    "        self.sgld_psnr_list = [] # psnr between sgld out and gt\n",
    "        self.MCMC_iter=50\n",
    "        self.param_noise_sigma=2\n",
    "\n",
    "        # tinker with image input\n",
    "        self.img_np = original_np           \n",
    "        self.img_noisy_np = noisy_np\n",
    "        self.img_noisy_torch = noisy_torch\n",
    "        \n",
    "        # network input\n",
    "        self.input_depth = 1\n",
    "        self.model = model.type(self.dtype)\n",
    "        self.net_input = get_noise(self.input_depth, 'noise', (img_np.shape[-2:][1], img_np.shape[-2:][0])).type(self.dtype).detach()\n",
    "        self.net_input_saved = self.net_input.detach().clone()\n",
    "        self.noise = self.net_input.detach().clone()\n",
    "        \n",
    "        # closure\n",
    "        self.reg_noise_std = tensor(1./30.)\n",
    "        self.criteria = torch.nn.MSELoss().type(dtype) # loss\n",
    "\n",
    "        # optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = 5e-8\n",
    "        \n",
    "        # burnin-end criteria\n",
    "        self.img_collection = []\n",
    "        self.variance_history = []\n",
    "        self.patience = patience\n",
    "        self.wait_count = 0\n",
    "        self.best_score = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        self.img_collection = []\n",
    "        self.burnin_over = False\n",
    "        self.buffer_size = buffer_size\n",
    "        self.cur_var = None\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        \"\"\"\n",
    "        We are doing a manual implementation of the SGLD optimizer\n",
    "        There is a SGLD optimizer that can be found here:\n",
    "            - https://pysgmcmc.readthedocs.io/en/pytorch/_modules/pysgmcmc/optimizers/sgld.html\n",
    "            - Implementing this would greatly affect the training step\n",
    "                - But could it work?? :`( I couldn't figure it out\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Trick this puppy into thinking we have a dataloader\n",
    "        It's a single image for deep image priors\n",
    "        So we just need to return a dataloader with a single image\n",
    "        \"\"\"\n",
    "        dataset = SingleImageDataset(self.img_np, self.num_iter)\n",
    "        return DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        \"\"\"\n",
    "        Move all tensors to the GPU to begin training\n",
    "        Initialize Iterators\n",
    "        Set Sail\n",
    "        \"\"\"\n",
    "        self.model.to(self.device)\n",
    "        self.net_input = self.net_input.to(self.device)\n",
    "        self.img_noisy_torch = self.img_noisy_torch.to(self.device)\n",
    "        self.reg_noise_std = self.reg_noise_std.to(self.device)\n",
    "\n",
    "        self.net_input_saved = self.net_input.clone().to(self.device)\n",
    "        self.noise = self.net_input.clone().to(self.device)\n",
    "        \n",
    "        # Initialize Iterations\n",
    "        self.i=0\n",
    "        self.sample_count=0\n",
    "\n",
    "        # bon voyage\n",
    "        print('Starting optimization with SGLD')\n",
    "\n",
    "    def forward(self, net_input_saved):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        occurs in the closure function in this implementation\n",
    "        \"\"\"\n",
    "        if self.reg_noise_std > 0:\n",
    "            self.net_input = self.net_input_saved + (self.noise.normal_() * self.reg_noise_std)\n",
    "            return self.model(self.net_input)\n",
    "        else:\n",
    "            return self.model(net_input_saved)\n",
    "\n",
    "    def update_burnin(self,out_np):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        check if we should end the burnin phase\n",
    "        \"\"\"\n",
    "        # update img collection\n",
    "        v_img_np = out_np.reshape(-1)\n",
    "        self.update_img_collection(v_img_np)\n",
    "        img_collection = self.get_img_collection()\n",
    "\n",
    "        if len(img_collection) >= self.buffer_size:\n",
    "            # update variance and var history\n",
    "            ave_img = np.mean(img_collection, axis=0)\n",
    "            variance = [self.MSE(ave_img, tmp) for tmp in img_collection]\n",
    "            self.cur_var = np.mean(variance)\n",
    "            self.variance_history.append(self.cur_var)\n",
    "            self.check_stop(self.cur_var, self.i)\n",
    "    \n",
    "    def backtracking(self, psrn_noisy, total_loss):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        backtracking to prevent oscillation if the PSNR is fluctuating\n",
    "        \"\"\"\n",
    "        if self.roll_back and self.i % self.show_every:\n",
    "            if psrn_noisy - self.psrn_noisy_last < -5: \n",
    "                print('Falling back to previous checkpoint.')\n",
    "                for new_param, net_param in zip(self.last_net, self.model.parameters()):\n",
    "                    net_param.detach().copy_(new_param.cuda())\n",
    "                return total_loss*0\n",
    "            else:\n",
    "                self.last_net = [x.detach().cpu() for x in model.parameters()]\n",
    "                self.psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    def closure_sgld(self):\n",
    "        out = self.forward(self.net_input)\n",
    "\n",
    "        # compute loss\n",
    "        total_loss = self.criteria(out, self.img_noisy_torch)\n",
    "        total_loss.backward()\n",
    "        out_np = out.detach().cpu().numpy()[0]\n",
    "\n",
    "        # compute PSNR\n",
    "        psrn_noisy = compare_psnr(self.img_noisy_np, out.detach().cpu().numpy()[0])\n",
    "        psrn_gt    = compare_psnr(self.img_np, out_np)\n",
    "        self.sgld_psnr_list.append(psrn_gt)\n",
    "\n",
    "        # early burn in termination criteria\n",
    "        if not self.burnin_over:\n",
    "            self.update_burnin(out_np)\n",
    "\n",
    "        # backtracking \n",
    "        self.backtracking(psrn_noisy, total_loss)\n",
    "        \n",
    "        # plot progress\n",
    "        if self.i % self.show_every == 0:\n",
    "            self.plot_progress(out_np, psrn_gt)\n",
    "\n",
    "        ##########################################\n",
    "        ### Logging and SGLD mean collection #####\n",
    "        ##########################################\n",
    "        \n",
    "        if self.burnin_over and np.mod(self.i, self.MCMC_iter) == 0:\n",
    "            self.sgld_mean += out_np\n",
    "            self.sample_count += 1.\n",
    "\n",
    "        if self.burnin_over:\n",
    "            self.burnin_iter+=1\n",
    "            self.sgld_mean_each += out_np\n",
    "            self.sgld_mean_tmp = self.sgld_mean_each / self.burnin_iter # (self.i - self.burnin_iter)\n",
    "            self.sgld_mean_psnr_each = compare_psnr(self.img_np, self.sgld_mean_tmp)\n",
    "            print('Iter: %d; psnr_gt %.2f; psnr_sgld %.2f' % (self.i, psrn_gt, self.sgld_mean_psnr_each))\n",
    "\n",
    "        elif self.cur_var is not None and not self.burnin_over:\n",
    "            print('Iter: %d; psnr_gt %.2f; loss %.5f; var %.8f' % (self.i, psrn_gt, total_loss, self.cur_var))\n",
    "\n",
    "        else:\n",
    "            print('Iter: %d; psnr_gt %.2f; loss %.5f' % (self.i, psrn_gt, total_loss))\n",
    "        \n",
    "        # if self.i == self.burnin_iter and self.burnin_over:\n",
    "        #     print('Burn-in done, start sampling')\n",
    "\n",
    "        self.i += 1\n",
    "        return total_loss\n",
    "\n",
    "    def add_noise(self, net):\n",
    "        \"\"\"\n",
    "        Add noise to the network parameters\n",
    "        This is the critical part of SGLD\n",
    "        \"\"\"\n",
    "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "            noise = torch.randn(n.size())*self.param_noise_sigma*self.learning_rate\n",
    "            noise = noise.type(dtype)\n",
    "            n.data = n.data + noise\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        \"\"\"\n",
    "        Oh the places you'll go\n",
    "        ---> Straight to error city calling this add_noise in the training step\n",
    "        ---> Consider using the on_train_batch_end hook? (each batch is only one iteration)\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizers()\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.closure_sgld()\n",
    "        optimizer.step()\n",
    "        self.add_noise(model)\n",
    "        return loss\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        \"\"\"\n",
    "        May all your dreams come true\n",
    "        \"\"\"\n",
    "        plotted = self.sgld_mean / self.sample_count\n",
    "        np_plot(plotted, 'Final after %d iterations' % (self.i), cmap='gray')\n",
    "\n",
    "    def check_stop(self, current, cur_epoch):\n",
    "        \"\"\"\n",
    "        using an early stopper technique to determine when to end the burn in phase for SGLD\n",
    "        https://arxiv.org/pdf/2112.06074.pdf\n",
    "        https://github.com/sun-umn/Early_Stopping_for_DIP/blob/main/ES_WMV.ipynb\n",
    "        \"\"\"\n",
    "        if current < self.best_score:\n",
    "            self.best_score = current\n",
    "            self.best_epoch = cur_epoch\n",
    "            self.wait_count = 0\n",
    "            self.burnin_over = False\n",
    "        else:\n",
    "            self.wait_count += 1\n",
    "            self.burnin_over = self.wait_count >= self.patience\n",
    "        if self.burnin_over:\n",
    "            print(f'\\n\\nBurn-in completed at iter {self.i}; \\nStarting SGLD Mean sampling;\\n\\n')\n",
    "            self.show_every = self.MCMC_iter\n",
    "\n",
    "    def update_img_collection(self, cur_img):\n",
    "        self.img_collection.append(cur_img)\n",
    "        if len(self.img_collection) > self.buffer_size:\n",
    "            self.img_collection.pop(0)\n",
    "\n",
    "    def get_img_collection(self):\n",
    "        return self.img_collection\n",
    "\n",
    "    def MSE(self, x1, x2):\n",
    "        return ((x1 - x2) ** 2).sum() / x1.size\n",
    "\n",
    "    def plot_progress(self, out_np, psrn_gt):\n",
    "        \"\"\"\n",
    "        plot original image\n",
    "        plot denoised image\n",
    "        plot noisy image\n",
    "\n",
    "        everything grayscaled\n",
    "        \"\"\"\n",
    "        if self.burnin_over and self.sample_count > 0:\n",
    "            plotted = self.sgld_mean / self.sample_count\n",
    "            #plotted = plotted.detach().cpu().numpy()[0]\n",
    "            label = \"SGLD mean\"\n",
    "        else:\n",
    "            plotted = out_np\n",
    "            label = \"Denoised image\"\n",
    "\n",
    "        _, self.ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "        self.ax[0].imshow(self.img_np.transpose(1, 2, 0), interpolation = 'nearest', cmap='gray')\n",
    "        self.ax[0].set_title('Original image')\n",
    "        self.ax[0].axis('off')\n",
    "\n",
    "        self.ax[1].imshow(plotted.transpose(1, 2, 0), interpolation = 'nearest', cmap='gray')\n",
    "        self.ax[1].set_title(label)\n",
    "        self.ax[1].axis('off')\n",
    "\n",
    "        self.ax[2].imshow(self.img_noisy_np.transpose(1, 2, 0), interpolation = 'nearest', cmap='gray')\n",
    "        self.ax[2].set_title('Noisy image')\n",
    "        self.ax[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor \n",
    "\n",
    "# choose iterations\n",
    "num_iter = 1500 # max iterations\n",
    "\n",
    "# get image\n",
    "# generate phantom stored in subfolder of parent directory\n",
    "resolution = 6\n",
    "max_depth = resolution - 1\n",
    "phantom = generate_phantom(resolution=resolution)\n",
    "raw_img_np = phantom.copy() # 1x64x64 np array    \n",
    "img_np = raw_img_np.copy() # 1x64x64 np array\n",
    "sigma=25/255\n",
    "# sigma = .05\n",
    "img_noisy_np = np.clip(img_np + np.random.normal(scale=sigma, size=img_np.shape), 0, 1).astype(np.float32)\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
    "\n",
    "# reference model \n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# Create the lightning module\n",
    "module = SGLD(\n",
    "        original_np=img_np,\n",
    "        noisy_np=img_noisy_np,\n",
    "        noisy_torch=img_noisy_torch,\n",
    "        learning_rate = 0.1, # .06\n",
    "        show_every=10,\n",
    "        patience=100,\n",
    "        buffer_size=200,\n",
    "        )\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = Trainer(\n",
    "            max_epochs=num_iter,\n",
    "            fast_dev_run=False,\n",
    "            gpus=1,\n",
    "            checkpoint_callback=False\n",
    "            )\n",
    "\n",
    "# Initialize ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./{lightning_logs}/{logger_name}/version_{version}/checkpoints/',\n",
    "    filename='{epoch}-{step}',\n",
    "    every_n_epochs=100,\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "# Add the checkpoint callback to trainer\n",
    "trainer.callbacks.append(checkpoint_callback)\n",
    "            \n",
    "if not hasattr(trainer, 'optimizer_frequencies'):\n",
    "    trainer.optimizer_frequencies = []\n",
    "\n",
    "# Create the lighting object for evaluator\n",
    "train_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "val_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "\n",
    "lightning = Lightning(lightning_module=module, trainer=trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "lightning.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phantom HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGLD Pytorch Lightning Module\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, image, num_iter):\n",
    "        self.image = image\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_iter\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Always return the same image (and maybe a noise tensor or other information if necessary??)\n",
    "        return self.image\n",
    "\n",
    "class SGLD_HPO(LightningModule):\n",
    "    def __init__(self, \n",
    "        original_np,\n",
    "        noisy_np,\n",
    "        noisy_torch,\n",
    "        learning_rate = 0.01,\n",
    "        show_every=20,\n",
    "        patience = 1000,\n",
    "        buffer_size = 100,\n",
    "        model=get_unet(),\n",
    "        weight_decay\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # iterators\n",
    "        self.burnin_iter=0 # burn-in iteration for SGLD\n",
    "        self.show_every=show_every\n",
    "        self.num_iter=20000\n",
    "\n",
    "        # backtracking\n",
    "        self.psrn_noisy_last=0\n",
    "        self.last_net = None\n",
    "        self.roll_back = True # To solve the oscillation of model training \n",
    "\n",
    "        # SGLD Output Accumulation\n",
    "        self.sgld_mean=0\n",
    "        self.sgld_mean_each=0\n",
    "        self.sgld_psnr_list = [] # psnr between sgld out and gt\n",
    "        self.MCMC_iter=50\n",
    "        self.param_noise_sigma=2\n",
    "\n",
    "        # tinker with image input\n",
    "        self.img_np = original_np           \n",
    "        self.img_noisy_np = noisy_np\n",
    "        self.img_noisy_torch = noisy_torch\n",
    "        \n",
    "        # network input\n",
    "        self.input_depth = 1\n",
    "        self.model = model.type(self.dtype)\n",
    "        self.net_input = get_noise(self.input_depth, 'noise', (img_np.shape[-2:][1], img_np.shape[-2:][0])).type(self.dtype).detach()\n",
    "        self.net_input_saved = self.net_input.detach().clone()\n",
    "        self.noise = self.net_input.detach().clone()\n",
    "        \n",
    "        # closure\n",
    "        self.reg_noise_std = tensor(1./30.)\n",
    "        self.criteria = torch.nn.MSELoss().type(dtype) # loss\n",
    "\n",
    "        # optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # burnin-end criteria\n",
    "        self.img_collection = []\n",
    "        self.variance_history = []\n",
    "        self.patience = patience\n",
    "        self.wait_count = 0\n",
    "        self.best_score = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        self.img_collection = []\n",
    "        self.burnin_over = False\n",
    "        self.buffer_size = buffer_size\n",
    "        self.cur_var = None\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        \"\"\"\n",
    "        We are doing a manual implementation of the SGLD optimizer\n",
    "        There is a SGLD optimizer that can be found here:\n",
    "            - https://pysgmcmc.readthedocs.io/en/pytorch/_modules/pysgmcmc/optimizers/sgld.html\n",
    "            - Implementing this would greatly affect the training step\n",
    "                - But could it work?? :`( I couldn't figure it out\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Trick this puppy into thinking we have a dataloader\n",
    "        It's a single image for deep image priors\n",
    "        So we just need to return a dataloader with a single image\n",
    "        \"\"\"\n",
    "        dataset = SingleImageDataset(self.img_np, self.num_iter)\n",
    "        return DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        \"\"\"\n",
    "        Move all tensors to the GPU to begin training\n",
    "        Initialize Iterators\n",
    "        Set Sail\n",
    "        \"\"\"\n",
    "        self.model.to(self.device)\n",
    "        self.net_input = self.net_input.to(self.device)\n",
    "        self.img_noisy_torch = self.img_noisy_torch.to(self.device)\n",
    "        self.reg_noise_std = self.reg_noise_std.to(self.device)\n",
    "\n",
    "        self.net_input_saved = self.net_input.clone().to(self.device)\n",
    "        self.noise = self.net_input.clone().to(self.device)\n",
    "        \n",
    "        # Initialize Iterations\n",
    "        self.i=0\n",
    "        self.sample_count=0\n",
    "\n",
    "        # bon voyage\n",
    "        print('Starting optimization with SGLD')\n",
    "\n",
    "    def forward(self, net_input_saved):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        occurs in the closure function in this implementation\n",
    "        \"\"\"\n",
    "        if self.reg_noise_std > 0:\n",
    "            self.net_input = self.net_input_saved + (self.noise.normal_() * self.reg_noise_std)\n",
    "            return self.model(self.net_input)\n",
    "        else:\n",
    "            return self.model(net_input_saved)\n",
    "\n",
    "    def update_burnin(self,out_np):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        check if we should end the burnin phase\n",
    "        \"\"\"\n",
    "        # update img collection\n",
    "        v_img_np = out_np.reshape(-1)\n",
    "        self.update_img_collection(v_img_np)\n",
    "        img_collection = self.get_img_collection()\n",
    "\n",
    "        if len(img_collection) >= self.buffer_size:\n",
    "            # update variance and var history\n",
    "            ave_img = np.mean(img_collection, axis=0)\n",
    "            variance = [self.MSE(ave_img, tmp) for tmp in img_collection]\n",
    "            self.cur_var = np.mean(variance)\n",
    "            self.variance_history.append(self.cur_var)\n",
    "            self.check_stop(self.cur_var, self.i)\n",
    "    \n",
    "    def backtracking(self, psrn_noisy, total_loss):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        backtracking to prevent oscillation if the PSNR is fluctuating\n",
    "        \"\"\"\n",
    "        if self.roll_back and self.i % self.show_every:\n",
    "            if psrn_noisy - self.psrn_noisy_last < -5: \n",
    "                print('Falling back to previous checkpoint.')\n",
    "                for new_param, net_param in zip(self.last_net, self.model.parameters()):\n",
    "                    net_param.detach().copy_(new_param.cuda())\n",
    "                return total_loss*0\n",
    "            else:\n",
    "                self.last_net = [x.detach().cpu() for x in model.parameters()]\n",
    "                self.psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    def closure_sgld(self):\n",
    "        out = self.forward(self.net_input)\n",
    "\n",
    "        # compute loss\n",
    "        total_loss = self.criteria(out, self.img_noisy_torch)\n",
    "        total_loss.backward()\n",
    "        out_np = out.detach().cpu().numpy()[0]\n",
    "\n",
    "        # compute PSNR\n",
    "        psrn_noisy = compare_psnr(self.img_noisy_np, out.detach().cpu().numpy()[0])\n",
    "        psrn_gt    = compare_psnr(self.img_np, out_np)\n",
    "        self.sgld_psnr_list.append(psrn_gt)\n",
    "\n",
    "        # early burn in termination criteria\n",
    "        if not self.burnin_over:\n",
    "            self.update_burnin(out_np)\n",
    "\n",
    "        # backtracking \n",
    "        self.backtracking(psrn_noisy, total_loss)\n",
    "        \n",
    "        # plot progress\n",
    "        if self.i % self.show_every == 0:\n",
    "            self.plot_progress(out_np, psrn_gt)\n",
    "\n",
    "        ##########################################\n",
    "        ### Logging and SGLD mean collection #####\n",
    "        ##########################################\n",
    "        \n",
    "        if self.burnin_over and np.mod(self.i, self.MCMC_iter) == 0:\n",
    "            self.sgld_mean += out_np\n",
    "            self.sample_count += 1.\n",
    "\n",
    "        if self.burnin_over:\n",
    "            self.burnin_iter+=1\n",
    "            self.sgld_mean_each += out_np\n",
    "            self.sgld_mean_tmp = self.sgld_mean_each / self.burnin_iter # (self.i - self.burnin_iter)\n",
    "            self.sgld_mean_psnr_each = compare_psnr(self.img_np, self.sgld_mean_tmp)\n",
    "            print('Iter: %d; psnr_gt %.2f; psnr_sgld %.2f' % (self.i, psrn_gt, self.sgld_mean_psnr_each))\n",
    "\n",
    "        elif self.cur_var is not None and not self.burnin_over:\n",
    "            print('Iter: %d; psnr_gt %.2f; loss %.5f; var %.8f' % (self.i, psrn_gt, total_loss, self.cur_var))\n",
    "\n",
    "        else:\n",
    "            print('Iter: %d; psnr_gt %.2f; loss %.5f' % (self.i, psrn_gt, total_loss))\n",
    "        \n",
    "        # if self.i == self.burnin_iter and self.burnin_over:\n",
    "        #     print('Burn-in done, start sampling')\n",
    "\n",
    "        self.i += 1\n",
    "        return total_loss\n",
    "\n",
    "    def add_noise(self, net):\n",
    "        \"\"\"\n",
    "        Add noise to the network parameters\n",
    "        This is the critical part of SGLD\n",
    "        \"\"\"\n",
    "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "            noise = torch.randn(n.size())*self.param_noise_sigma*self.learning_rate\n",
    "            noise = noise.type(dtype)\n",
    "            n.data = n.data + noise\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        \"\"\"\n",
    "        Oh the places you'll go\n",
    "        ---> Straight to error city calling this add_noise in the training step\n",
    "        ---> Consider using the on_train_batch_end hook? (each batch is only one iteration)\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizers()\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.closure_sgld()\n",
    "        optimizer.step()\n",
    "        self.add_noise(model)\n",
    "        return loss\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        \"\"\"\n",
    "        May all your dreams come true\n",
    "        \"\"\"\n",
    "        plotted = self.sgld_mean / self.sample_count\n",
    "        np_plot(plotted, 'Final after %d iterations' % (self.i), cmap='gray')\n",
    "\n",
    "    def check_stop(self, current, cur_epoch):\n",
    "        \"\"\"\n",
    "        using an early stopper technique to determine when to end the burn in phase for SGLD\n",
    "        https://arxiv.org/pdf/2112.06074.pdf\n",
    "        https://github.com/sun-umn/Early_Stopping_for_DIP/blob/main/ES_WMV.ipynb\n",
    "        \"\"\"\n",
    "        if current < self.best_score:\n",
    "            self.best_score = current\n",
    "            self.best_epoch = cur_epoch\n",
    "            self.wait_count = 0\n",
    "            self.burnin_over = False\n",
    "        else:\n",
    "            self.wait_count += 1\n",
    "            self.burnin_over = self.wait_count >= self.patience\n",
    "        if self.burnin_over:\n",
    "            print(f'\\n\\nBurn-in completed at iter {self.i}; \\nStarting SGLD Mean sampling;\\n\\n')\n",
    "            self.show_every = self.MCMC_iter\n",
    "\n",
    "    def update_img_collection(self, cur_img):\n",
    "        self.img_collection.append(cur_img)\n",
    "        if len(self.img_collection) > self.buffer_size:\n",
    "            self.img_collection.pop(0)\n",
    "\n",
    "    def get_img_collection(self):\n",
    "        return self.img_collection\n",
    "\n",
    "    def MSE(self, x1, x2):\n",
    "        return ((x1 - x2) ** 2).sum() / x1.size\n",
    "\n",
    "    def plot_progress(self, out_np, psrn_gt):\n",
    "        \"\"\"\n",
    "        plot original image\n",
    "        plot denoised image\n",
    "        plot noisy image\n",
    "\n",
    "        everything grayscaled\n",
    "        \"\"\"\n",
    "        if self.burnin_over and self.sample_count > 0:\n",
    "            plotted = self.sgld_mean / self.sample_count\n",
    "            #plotted = plotted.detach().cpu().numpy()[0]\n",
    "            label = \"SGLD mean\"\n",
    "        else:\n",
    "            plotted = out_np\n",
    "            label = \"Denoised image\"\n",
    "\n",
    "        _, self.ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "        self.ax[0].imshow(self.img_np.transpose(1, 2, 0), interpolation = 'nearest', cmap='gray')\n",
    "        self.ax[0].set_title('Original image')\n",
    "        self.ax[0].axis('off')\n",
    "\n",
    "        self.ax[1].imshow(plotted.transpose(1, 2, 0), interpolation = 'nearest', cmap='gray')\n",
    "        self.ax[1].set_title(label)\n",
    "        self.ax[1].axis('off')\n",
    "\n",
    "        self.ax[2].imshow(self.img_noisy_np.transpose(1, 2, 0), interpolation = 'nearest', cmap='gray')\n",
    "        self.ax[2].set_title('Noisy image')\n",
    "        self.ax[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor \n",
    "\n",
    "# choose iterations\n",
    "num_iter = 1500 # max iterations\n",
    "\n",
    "# get image\n",
    "# generate phantom stored in subfolder of parent directory\n",
    "resolution = 6\n",
    "max_depth = resolution - 1\n",
    "phantom = generate_phantom(resolution=resolution)\n",
    "raw_img_np = phantom.copy() # 1x64x64 np array    \n",
    "img_np = raw_img_np.copy() # 1x64x64 np array\n",
    "sigma=25/255\n",
    "# sigma = .05\n",
    "img_noisy_np = np.clip(img_np + np.random.normal(scale=sigma, size=img_np.shape), 0, 1).astype(np.float32)\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
    "\n",
    "# reference model \n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# Create the lightning module\n",
    "module = SGLD(\n",
    "        original_np=img_np,\n",
    "        noisy_np=img_noisy_np,\n",
    "        noisy_torch=img_noisy_torch,\n",
    "        learning_rate = 0.1, # .06\n",
    "        show_every=10,\n",
    "        patience=100,\n",
    "        buffer_size=200,\n",
    "        )\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = Trainer(\n",
    "            max_epochs=num_iter,\n",
    "            fast_dev_run=False,\n",
    "            gpus=1,\n",
    "            checkpoint_callback=False\n",
    "            )\n",
    "\n",
    "# Initialize ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./{lightning_logs}/{logger_name}/version_{version}/checkpoints/',\n",
    "    filename='{epoch}-{step}',\n",
    "    every_n_epochs=100,\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "# Add the checkpoint callback to trainer\n",
    "trainer.callbacks.append(checkpoint_callback)\n",
    "            \n",
    "if not hasattr(trainer, 'optimizer_frequencies'):\n",
    "    trainer.optimizer_frequencies = []\n",
    "\n",
    "# Create the lighting object for evaluator\n",
    "train_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "val_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "\n",
    "lightning = Lightning(lightning_module=module, trainer=trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "lightning.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nni\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'patience': 1000,\n",
    "        'buffer_size': 100,\n",
    "        'weight_decay': 5e-8, # this is proportionate to a 1024x1024 image\n",
    "        }\n",
    "\n",
    "optimized_params = nni.get_next_parameter()\n",
    "params.update(optimized_params)\n",
    "print(params)\n",
    "\n",
    "train_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "val_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "\n",
    "module = SGLD_HPO(\n",
    "        original_np=img_np,\n",
    "        noisy_np=img_noisy_np,\n",
    "        noisy_torch=img_noisy_torch,\n",
    "        learning_rate = params['learning_rate']\n",
    "        show_every=500,\n",
    "        patience=params['patience'],\n",
    "        buffer_size= params['buffer_size'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
