{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nni\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nni.retiarii.nn.pytorch as nn\n",
    "import nni.retiarii.strategy as strategy\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from nni.experiment import Experiment\n",
    "from nni.retiarii import model_wrapper\n",
    "from nni.retiarii.evaluator import FunctionalEvaluator\n",
    "from nni.retiarii.experiment.pytorch import RetiariiExperiment, RetiariiExeConfig\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as skimage_psnr\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model_wrapper\n",
    "class SearchSpace(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_channels = 64\n",
    "        ks = nn.ValueChoice([1, 3], label=\"Kernel Size\")\n",
    "        dl = nn.ValueChoice([1, 3, 5], label=\"Dilation Rate\")\n",
    "        pd = (ks-1)*dl//2\n",
    "\n",
    "        activations = OrderedDict([\n",
    "            (\"RelU\", nn.ReLU(inplace=True)),\n",
    "            (\"LeakyRelU\", nn.LeakyReLU(inplace=True)),\n",
    "            (\"Sigmoid\", nn.Sigmoid()),\n",
    "            (\"Selu\", nn.SELU(inplace=True)),\n",
    "            (\"PreLU\", nn.PReLU()),\n",
    "            (\"SiLU\", nn.SiLU(inplace=True)),\n",
    "        ])\n",
    "\n",
    "        downsample = OrderedDict([\n",
    "            (\"AvgPool2d\", nn.AvgPool2d(kernel_size=2, stride=2)),\n",
    "            (\"MaxPool2d\", nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "        ])\n",
    "\n",
    "        upsample = OrderedDict([\n",
    "            (\"Nearest\", nn.Upsample(scale_factor=2,mode='nearest')),\n",
    "            (\"Bilinear\", nn.Upsample(scale_factor=2,mode='bilinear', align_corners=True)),\n",
    "            (\"Bicubic\", nn.Upsample(scale_factor=2,mode='bicubic', align_corners=True))\n",
    "        ])\n",
    "\n",
    "        # Conv layer in\"\n",
    "        self.layer1_name = \"1 - Conv layer in\"\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
    "        self.act1 = nn.LayerChoice(activations, label=f\"{self.layer1_name}: Activation Function 1\")\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_channels)\n",
    "        self.act2 = nn.LayerChoice(activations, label=f\"{self.layer1_name}: Activation Function 2\")\n",
    "\n",
    "        self.first = nn.Sequential(\n",
    "            self.conv1,self.bn1,self.act1,\n",
    "            self.conv2, self.bn2,self.act2\n",
    "        )\n",
    "\n",
    "        # Encoder 1\n",
    "        self.layer2_name = \"2 - Encoder 1\"\n",
    "        self.downsample1 = nn.LayerChoice(downsample,label=\"First Pooling Technique\")\n",
    "\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_channels*2)\n",
    "        self.act3 = nn.LayerChoice(activations, label=f\"{self.layer2_name}: Activation Function 1\")\n",
    "        self.conv4 = nn.Conv2d(hidden_channels*2, hidden_channels*2, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_channels*2)\n",
    "        self.act4 = nn.LayerChoice(activations, label=f\"{self.layer2_name}: Activation Function 2\")\n",
    "\n",
    "        self.Encoder1 = nn.Sequential(\n",
    "            self.downsample1,\n",
    "            self.conv3,self.bn3,self.act3,\n",
    "            self.conv4,self.bn4,self.act4\n",
    "        )\n",
    "\n",
    "        # Decoder 1\n",
    "        self.layer3_name = \"3 - Decoder 1\"\n",
    "        self.upsample1 = nn.LayerChoice(upsample, label=\"First Upsample Technique\")\n",
    "\n",
    "        self.conv5 = nn.Conv2d(hidden_channels*3, hidden_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn5 = nn.BatchNorm2d(hidden_channels)\n",
    "        self.act5 = nn.LayerChoice(activations, label=f\"{self.layer3_name}: Activation Function 1\")\n",
    "        self.conv6 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn6 = nn.BatchNorm2d(hidden_channels)\n",
    "        self.act6 = nn.LayerChoice(activations, label=f\"{self.layer3_name}: Activation Function 2\")\n",
    "\n",
    "        self.Decoder1 = nn.Sequential(\n",
    "            self.conv5, self.bn5, self.act5,\n",
    "            self.conv6, self.bn6, self.act6\n",
    "            )\n",
    "\n",
    "        # Conv layer out\n",
    "        self.out = nn.Conv2d(hidden_channels, out_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "    \n",
    "    def crop_tensor(self, target_tensor, tensor):\n",
    "        target_size = target_tensor.size()[2]  # Assuming height and width are same\n",
    "        tensor_size = tensor.size()[2]\n",
    "        delta = tensor_size - target_size\n",
    "        delta = delta // 2\n",
    "        return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "    \n",
    "    def upsample_and_crop(self, input, skip, upsample, decode):\n",
    "        upsampled = upsample(input)\n",
    "        cropped = self.crop_tensor(upsampled, skip)\n",
    "        return decode(torch.cat([cropped, upsampled], 1))\n",
    "\n",
    "    def print_info(self, layer_name, tensor):\n",
    "        separator = \"-\" * 50\n",
    "        if layer_name == \"Input\":\n",
    "            print(separator)\n",
    "            print(separator)\n",
    "        if layer_name == \"Output\":\n",
    "            print(f\"Layer: {layer_name} -- Conv Layer Out\")\n",
    "            print(f\"Output Shape: {tensor.shape}\")\n",
    "            print(separator)\n",
    "            print(separator)\n",
    "        else:        \n",
    "            print(f\"Layer: {layer_name}\")\n",
    "            print(f\"Output Shape: {tensor.shape}\")\n",
    "            print(separator)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.print_info(\"Input\", x)\n",
    "\n",
    "        x = self.first(x)\n",
    "        self.print_info(\"Conv layer in\", x)\n",
    "\n",
    "        x1 = self.Encoder1(x)\n",
    "        self.print_info(\"Encoder 1\", x1)\n",
    "\n",
    "        x2 = self.upsample_and_crop(\n",
    "            x1, # input\n",
    "            x, # skip\n",
    "            self.upsample1, # upsample \n",
    "            self.Decoder1 # decode\n",
    "            )\n",
    "        self.print_info(\"Decoder 1\", x2)\n",
    "\n",
    "        x3 = self.out(x2)\n",
    "        self.print_info(\"Output\", x3)\n",
    "\n",
    "        return x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(image_true, image_test):\n",
    "    # Convert PyTorch tensors to NumPy arrays\n",
    "    if torch.is_tensor(image_true):\n",
    "        image_true = image_true.detach().cpu().numpy()\n",
    "    if torch.is_tensor(image_test):\n",
    "        image_test = image_test.detach().cpu().numpy()\n",
    "    return skimage_psnr(image_true, image_test)\n",
    "\n",
    "def add_noise(img, noise_factor=0.5):\n",
    "    \"\"\"Add random noise to an image.\"\"\"\n",
    "    noise = torch.randn_like(img) * noise_factor\n",
    "    noisy_img = img + noise\n",
    "    return torch.clamp(noisy_img, 0., 1.)\n",
    "\n",
    "def deep_image_prior_denoising(model, noisy_img, clean_img, device, optimizer, iterations=3000):\n",
    "    model.train()\n",
    "    for iteration in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.randn(noisy_img.shape).to(device))\n",
    "        loss = nn.MSELoss()(output, noisy_img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % 250 == 0:\n",
    "            # Calculate PSNR\n",
    "            with torch.no_grad():\n",
    "                denoised_output = model(noisy_img)\n",
    "                psnr_value = psnr(clean_img, denoised_output)\n",
    "            print('Iteration: {}\\tLoss: {:.6f}\\tPSNR: {:.6f} dB'.format(iteration, loss.item(), psnr_value))\n",
    "            nni.report_intermediate_result(psnr_value)\n",
    "    return output\n",
    "\n",
    "def evaluate_denoising(denoised_img, clean_img):\n",
    "    # We no longer need the model in an eval state or any forward pass here\n",
    "    # because the denoised image is already generated and passed to the function.\n",
    "    return psnr(clean_img, denoised_img)\n",
    "\n",
    "def main_evaluation(model_cls):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = model_cls().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    img, _ = dataset[0]  # Original clean image\n",
    "    noisy_img = add_noise(img).unsqueeze(0).to(device)  # Noisy version of image\n",
    "\n",
    "    # Denoise the image for a set number of iterations\n",
    "    denoised_img = deep_image_prior_denoising(model, noisy_img, img.unsqueeze(0).to(device), device, optimizer)\n",
    "\n",
    "    # Evaluate the PSNR of the denoised image\n",
    "    psnr_value = evaluate_denoising(denoised_img, img.unsqueeze(0).to(device))\n",
    "    print('PSNR: {:.6f} dB'.format(psnr_value))\n",
    "\n",
    "    # Report final PSNR to NNI\n",
    "    nni.report_final_result(psnr_value.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutions(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ks, pd, dl, activations, layer_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.act1 = nn.LayerChoice(activations, label=f'{layer_name}: activation 1')\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.act2 = nn.LayerChoice(activations, label=f'{layer_name}: activation 2')\n",
    "\n",
    "        self.seq = nn.Sequential(self.conv1, self.bn1, self.act1, self.conv2, self.bn2, self.act2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "@model_wrapper\n",
    "class SearchSpace(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_channels = 64\n",
    "        ks = nn.ValueChoice([1, 3], label=\"Kernel Size\")\n",
    "        dl = nn.ValueChoice([1, 3, 5], label=\"Dilation Rate\")\n",
    "        pd = (ks-1)*dl//2\n",
    "        pdd = ks // 2\n",
    "\n",
    "        activations = OrderedDict([\n",
    "            (\"RelU\", nn.ReLU(inplace=True)),\n",
    "            (\"LeakyRelU\", nn.LeakyReLU(inplace=True)),\n",
    "            (\"Sigmoid\", nn.Sigmoid()),\n",
    "            (\"Selu\", nn.SELU(inplace=True)),\n",
    "            (\"PreLU\", nn.PReLU()),\n",
    "            (\"SiLU\", nn.SiLU(inplace=True)),\n",
    "        ])\n",
    "\n",
    "        downsample = OrderedDict([\n",
    "            (\"AvgPool2d\", nn.AvgPool2d(kernel_size=2, stride=2)),\n",
    "            (\"MaxPool2d\", nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "        ])\n",
    "\n",
    "        upsample = OrderedDict([\n",
    "            (\"Nearest\", nn.Upsample(scale_factor=2,mode='nearest')),\n",
    "            (\"Bilinear\", nn.Upsample(scale_factor=2,mode='bilinear', align_corners=True)),\n",
    "            (\"Bicubic\", nn.Upsample(scale_factor=2,mode='bicubic', align_corners=True))\n",
    "        ])\n",
    "\n",
    "        encoderconvs1 = OrderedDict([\n",
    "            (\"Conv2d\", nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=ks, padding=pd, dilation=dl)),\n",
    "            (\"DepthwiseSeparable\", nn.Sequential(\n",
    "                nn.Conv2d(hidden_channels, hidden_channels, kernel_size=ks, padding=pdd, dilation=dl, groups=hidden_channels),\n",
    "                nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=1, padding=0, dilation=1)\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # encoderconvs2 = OrderedDict([\n",
    "        #     (\"Conv2d\", nn.Conv2d(hidden_channels*2, hidden_channels*2, kernel_size=ks, padding=pd, dilation=dl)),\n",
    "        #     (\"DepthwiseSeparable\", nn.Sequential(\n",
    "        #         nn.Conv2d(hidden_channels*2, hidden_channels*2, kernel_size=ks, padding=pdd, dilation=dl, groups=hidden_channels),\n",
    "        #         nn.Conv2d(hidden_channels*2, hidden_channels*2, kernel_size=1, padding=0, dilation=1)\n",
    "        #         )\n",
    "        #     ),\n",
    "        #     (\"Depthwise\", nn.Conv2d(hidden_channels*2, hidden_channels*2, kernel_size=1, padding=0, dilation=1, groups=hidden_channels))\n",
    "        # ])\n",
    "\n",
    "        # decoderconvs1 = OrderedDict([\n",
    "        #     (\"Conv2d\", nn.Conv2d(hidden_channels*3, hidden_channels, kernel_size=ks, padding=pd, dilation=dl)),\n",
    "        #     (\"DepthwiseSeparable\", nn.Sequential(\n",
    "        #         nn.Conv2d(hidden_channels*3, hidden_channels*3, kernel_size=ks, padding=pdd, dilation=dl, groups=hidden_channels*3),\n",
    "        #         nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, padding=0, dilation=1)\n",
    "        #         )\n",
    "        #     )\n",
    "        # ])\n",
    "\n",
    "        # decoderconvs2 = OrderedDict([\n",
    "        #     (\"Conv2d\", nn.Conv2d(hidden_channels, hidden_channels, kernel_size=ks, padding=pd, dilation=dl)),\n",
    "        #     (\"DepthwiseSeparable\", nn.Sequential(\n",
    "        #         nn.Conv2d(hidden_channels, hidden_channels, kernel_size=ks, padding=pdd, dilation=dl, groups=hidden_channels),\n",
    "        #         nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, padding=0, dilation=1)\n",
    "        #         )\n",
    "        #     ),\n",
    "        #     (\"Depthwise\", nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, padding=0, dilation=1, groups=hidden_channels))\n",
    "        # ])\n",
    "\n",
    "        # Conv layer in\"\n",
    "        self.first = Convolutions(in_channels, hidden_channels, ks, pd, dl, activations, \"1 - Conv layer in\")\n",
    "\n",
    "        # Encoder 1\n",
    "        self.layer2_name = \"2 - Encoder 1\"\n",
    "        self.downsample1 = nn.LayerChoice(downsample,label=f'{self.layer2_name}: Downsampling Technique')\n",
    "        self.Encoder1 = nn.Sequential(\n",
    "            self.downsample1,\n",
    "            Convolutions(hidden_channels, hidden_channels*2, ks, pd, dl, activations, self.layer2_name),\n",
    "        )\n",
    "\n",
    "        # Decoder 1\n",
    "        self.layer3_name = \"3 - Decoder 1\"\n",
    "        self.upsample1 = nn.LayerChoice(upsample, label=f\"{self.layer3_name}: Upsampling Technique\")\n",
    "        self.Decoder1 = Convolutions(hidden_channels*3, hidden_channels, ks, pd, dl, activations, self.layer3_name)\n",
    "\n",
    "        # Conv layer out\n",
    "        self.out = nn.Conv2d(hidden_channels, out_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "    \n",
    "    def crop_tensor(self, target_tensor, tensor):\n",
    "        target_size = target_tensor.size()[2]  # Assuming height and width are same\n",
    "        tensor_size = tensor.size()[2]\n",
    "        delta = tensor_size - target_size\n",
    "        delta = delta // 2\n",
    "        return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "    \n",
    "    def upsample_and_crop(self, input, skip, upsample, decode):\n",
    "        upsampled = upsample(input)\n",
    "        cropped = self.crop_tensor(upsampled, skip)\n",
    "        return decode(torch.cat([cropped, upsampled], 1))\n",
    "\n",
    "    def print_info(self, layer_name, tensor):\n",
    "        separator = \"-\" * 50\n",
    "        if layer_name == \"Input\":\n",
    "            print(separator)\n",
    "            print(separator)\n",
    "        if layer_name == \"Output\":\n",
    "            print(f\"Layer: {layer_name} -- Conv Layer Out\")\n",
    "            print(f\"Output Shape: {tensor.shape}\")\n",
    "            print(separator)\n",
    "            print(separator)\n",
    "        else:        \n",
    "            print(f\"Layer: {layer_name}\")\n",
    "            print(f\"Output Shape: {tensor.shape}\")\n",
    "            print(separator)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.print_info(\"Input\", x)\n",
    "\n",
    "        x = self.first(x)\n",
    "        self.print_info(\"Conv layer in\", x)\n",
    "\n",
    "        x1 = self.Encoder1(x)\n",
    "        self.print_info(\"Encoder 1\", x1)\n",
    "\n",
    "        x2 = self.upsample_and_crop(\n",
    "            x1, # input\n",
    "            x, # skip\n",
    "            self.upsample1, # upsample \n",
    "            self.Decoder1 # decode\n",
    "            )\n",
    "        self.print_info(\"Decoder 1\", x2)\n",
    "\n",
    "        x3 = self.out(x2)\n",
    "        self.print_info(\"Output\", x3)\n",
    "\n",
    "        return x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space\n",
    "model_space = SearchSpace()\n",
    "evaluator = FunctionalEvaluator(main_evaluation)\n",
    "\n",
    "# search strategy\n",
    "search_strategy = strategy.Random(dedup=True)\n",
    "\n",
    "# experiment\n",
    "exp = RetiariiExperiment(model_space, evaluator, [], search_strategy)\n",
    "exp_config = RetiariiExeConfig('local')\n",
    "exp_config.experiment_name = 'mnist_search'\n",
    "exp_config.trial_code_directory = 'C:/Users/Public/Public_VS_Code/NAS_test'\n",
    "exp_config.experiment_working_directory = 'C:/Users/Public/nni-experiments'\n",
    "\n",
    "exp_config.max_trial_number = 1200   # spawn 4 trials at most\n",
    "exp_config.trial_concurrency = 2  # will run two trials concurrently\n",
    "\n",
    "exp_config.trial_gpu_number = 1\n",
    "exp_config.training_service.use_active_gpu = True\n",
    "\n",
    "# Execute\n",
    "exp.run(exp_config, 8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.connect(8081)\n",
    "experiment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutions(nn.Module):\n",
    "    def __init__(self, out_channels, activations, convs1, convs2, layer_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.LayerChoice(convs1, label=f'{layer_name} - Step 1: convolution 1')\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.act1 = nn.LayerChoice(activations, label=f'{layer_name} - Step 2: activation 1')\n",
    "        \n",
    "        self.conv2 = nn.LayerChoice(convs2, label=f'{layer_name} - Step 3: convolution 2')\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.act2 = nn.LayerChoice(activations, label=f'{layer_name} - Step 4: activation 2')\n",
    "\n",
    "        self.seq = nn.Sequential(self.conv1, self.bn1, self.act1, self.conv2, self.bn2, self.act2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "@model_wrapper\n",
    "class SearchSpace(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        ks = nn.ValueChoice([1, 5], label=\"Kernel Size\")\n",
    "        dl = nn.ValueChoice([3], label=\"Dilation Rate\")\n",
    "        pd = (ks - 1) * dl // 2\n",
    "        # pdd = ks // 2\n",
    "\n",
    "        activations = OrderedDict([\n",
    "            (\"RelU\", nn.ReLU(inplace=True)),\n",
    "            # (\"LeakyRelU\", nn.LeakyReLU(inplace=True)),\n",
    "            # (\"Sigmoid\", nn.Sigmoid()),\n",
    "            # (\"Selu\", nn.SELU(inplace=True)),\n",
    "            # (\"PreLU\", nn.PReLU()),\n",
    "            # (\"SiLU\", nn.SiLU(inplace=True)),\n",
    "        ])\n",
    "\n",
    "        downsamples = OrderedDict([\n",
    "            (\"AvgPool2d\", nn.AvgPool2d(kernel_size=2, stride=2)),\n",
    "            # (\"MaxPool2d\", nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "        ])\n",
    "\n",
    "        upsamples = OrderedDict([\n",
    "            (\"Nearest\", nn.Upsample(scale_factor=2,mode='nearest')),\n",
    "            # (\"Bilinear\", nn.Upsample(scale_factor=2,mode='bilinear', align_corners=True)),\n",
    "            # (\"Bicubic\", nn.Upsample(scale_factor=2,mode='bicubic', align_corners=True))\n",
    "        ])\n",
    "\n",
    "        # Conv layer in\"\n",
    "        self.layer1_out_channels = 64\n",
    "        self.convs1 = self.get_conv_ordered_dict(in_channels, self.layer1_out_channels, ks, pd, dl)\n",
    "        self.convs2 = self.get_conv_ordered_dict(self.layer1_out_channels, self.layer1_out_channels, ks, pd, dl, first=False)\n",
    "        self.first = Convolutions(self.layer1_out_channels, activations, self.convs1, self.convs2, \"1 - Conv layer in\")\n",
    "\n",
    "        # Encoder 1\n",
    "        self.layer2_name = \"2 - Encoder 1\"\n",
    "        self.layer2_in_channels = 64\n",
    "        self.layer2_out_channels = 128\n",
    "        self.encoderconvs1 = self.get_conv_ordered_dict(self.layer2_in_channels, self.layer2_out_channels, ks, pd, dl)\n",
    "        self.encoderconvs2 = self.get_conv_ordered_dict(self.layer2_out_channels, self.layer2_out_channels, ks, pd, dl, first=False)\n",
    "        self.downsample1 = nn.LayerChoice(downsamples,label=f'{self.layer2_name}: Downsampling Technique')\n",
    "        self.Encoder1 = nn.Sequential(\n",
    "            self.downsample1,\n",
    "            Convolutions(self.layer2_out_channels, activations, self.encoderconvs1, self.encoderconvs2, self.layer2_name),\n",
    "        )\n",
    "\n",
    "        # Decoder 1\n",
    "        self.layer3_name = \"3 - Decoder 1\"\n",
    "        self.layer3_in_channels = 64*3\n",
    "        self.layer3_out_channels = 64\n",
    "        self.decoderconvs1 = self.get_conv_ordered_dict(self.layer3_in_channels, self.layer3_out_channels, ks, pd, dl)\n",
    "        self.decoderconvs2 = self.get_conv_ordered_dict(self.layer3_out_channels, self.layer3_out_channels, ks, pd, dl, first=False)\n",
    "        self.upsample1 = nn.LayerChoice(upsamples, label=f\"{self.layer3_name}: Upsampling Technique\")\n",
    "        self.Decoder1 = Convolutions(self.layer3_out_channels, activations, self.decoderconvs1, self.decoderconvs2, self.layer3_name)\n",
    "\n",
    "        # Conv layer out\n",
    "        self.out = nn.Conv2d(self.layer3_out_channels, out_channels, kernel_size=ks, padding=pd, dilation=dl)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.print_info(\"Input\", x)\n",
    "\n",
    "        x = self.first(x)\n",
    "        self.print_info(\"Conv layer in\", x)\n",
    "\n",
    "        x1 = self.Encoder1(x)\n",
    "        self.print_info(\"Encoder 1\", x1)\n",
    "\n",
    "        x2 = self.upsample_and_crop(\n",
    "            x1, # input\n",
    "            x, # skip\n",
    "            self.upsample1, # upsample \n",
    "            self.Decoder1 # decode\n",
    "            )\n",
    "        self.print_info(\"Decoder 1\", x2)\n",
    "\n",
    "        x3 = self.out(x2)\n",
    "        self.print_info(\"Output\", x3)\n",
    "\n",
    "        return x3\n",
    "    \n",
    "    def get_conv_ordered_dict(self, in_channels, out_channels, ks, pd, dl, first=False):\n",
    "         layers = [\n",
    "                (\"Conv2d\", nn.Conv2d(in_channels, out_channels, kernel_size=ks, padding=pd, dilation=dl)),\n",
    "                (\"DepthwiseSeparable\", nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, in_channels, kernel_size=ks, padding=pd, dilation=dl, groups=in_channels),\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, dilation=1)\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "         if not first:\n",
    "            layers.append((\"Depthwise\", nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, dilation=1)))\n",
    "         return OrderedDict(layers)\n",
    "\n",
    "    def crop_tensor(self, target_tensor, tensor):\n",
    "        target_size = target_tensor.size()[2]  # Assuming height and width are same\n",
    "        tensor_size = tensor.size()[2]\n",
    "        delta = tensor_size - target_size\n",
    "        delta = delta // 2\n",
    "        return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "    \n",
    "    def upsample_and_crop(self, input, skip, upsample, decode):\n",
    "        upsampled = upsample(input)\n",
    "        cropped = self.crop_tensor(upsampled, skip)\n",
    "        return decode(torch.cat([cropped, upsampled], 1))\n",
    "\n",
    "    def print_info(self, layer_name, tensor):\n",
    "        separator = \"-\" * 50\n",
    "        if layer_name == \"Input\":\n",
    "            print(separator)\n",
    "            print(separator)\n",
    "        if layer_name == \"Output\":\n",
    "            print(f\"Layer: {layer_name} -- Conv Layer Out\")\n",
    "            print(f\"Output Shape: {tensor.shape}\")\n",
    "            print(separator)\n",
    "            print(separator)\n",
    "        else:        \n",
    "            print(f\"Layer: {layer_name}\")\n",
    "            print(f\"Output Shape: {tensor.shape}\")\n",
    "            print(separator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space\n",
    "model_space = SearchSpace()\n",
    "evaluator = FunctionalEvaluator(main_evaluation)\n",
    "\n",
    "# search strategy\n",
    "search_strategy = strategy.Random(dedup=True)\n",
    "\n",
    "# experiment\n",
    "exp = RetiariiExperiment(model_space, evaluator, [], search_strategy)\n",
    "exp_config = RetiariiExeConfig('local')\n",
    "exp_config.experiment_name = 'mnist_search'\n",
    "exp_config.trial_code_directory = 'C:/Users/Public/Public_VS_Code/NAS_test'\n",
    "exp_config.experiment_working_directory = 'C:/Users/Public/nni-experiments'\n",
    "\n",
    "exp_config.max_trial_number = 1200   # spawn 4 trials at most\n",
    "exp_config.trial_concurrency = 2  # will run two trials concurrently\n",
    "\n",
    "exp_config.trial_gpu_number = 1\n",
    "exp_config.training_service.use_active_gpu = True\n",
    "\n",
    "# Execute\n",
    "exp.run(exp_config, 8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.connect(8081)\n",
    "experiment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pub_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
