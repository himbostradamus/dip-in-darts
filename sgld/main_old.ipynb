{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.experiment import Experiment\n",
    "\n",
    "search_space = {\n",
    "    'learning_rate': {'_type': 'loguniform', '_value': [0.005, 0.2]},\n",
    "    'patience': {'_type': 'uniform', '_value': [500, 2000]},\n",
    "    'buffer_size': {'_type': 'choice', '_value': [100, 250, 500]},\n",
    "    'weight_decay': {'_type': 'loguniform', '_value': [1e-8, 1e-6]},\n",
    "}\n",
    "\n",
    "experiment = Experiment('local')\n",
    "\n",
    "experiment.config.trial_command = 'python model.py'\n",
    "experiment.config.trial_code_directory = '.'\n",
    "\n",
    "experiment.config.search_space = search_space\n",
    "\n",
    "experiment.config.tuner.name = 'TPE'\n",
    "experiment.config.tuner.class_args['optimize_mode'] = 'maximize'\n",
    "\n",
    "# experiment.config.max_trial_number = 10\n",
    "experiment.config.trial_concurrency = 1\n",
    "# experiment.config.num_workers = 16\n",
    "\n",
    "experiment.run(port=8889)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To kill an experiment from the command line use the following command:\n",
    "# lsof -i | grep :8889 | awk '{print $2}' | xargs kill -9\n",
    "\n",
    "# input('Press enter to quit')\n",
    "experiment.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ian try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the HPO Search Space\n",
    "\n",
    "import nni\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset #, DataLoader\n",
    "from torch import tensor\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from utils.common_utils import *\n",
    "\n",
    "from phantom import generate_phantom\n",
    "\n",
    "# from nni.retiarii.evaluator.pytorch import Lightning, Trainer, LightningModule\n",
    "# from nni.retiarii.evaluator.pytorch.lightning import DataLoader\n",
    "\n",
    "from nni.nas.evaluator.pytorch import Lightning, Trainer, LightningModule\n",
    "from nni.nas.evaluator.pytorch.lightning import DataLoader\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "### NAS in DARTS ###\n",
    "# from nni.nas.experiment import NasExperiment\n",
    "# from nni.nas.strategy import DARTS as DartsStrategy\n",
    "# from HPO_space import DARTS_UNet\n",
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "\n",
    "def get_unet():\n",
    "    return torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# SGLD Pytorch Lightning Module\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, image, num_iter):\n",
    "        self.image = image\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_iter\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Always return the same image (and maybe a noise tensor or other information if necessary??)\n",
    "        return self.image\n",
    "\n",
    "class SGLD_HPO(LightningModule):\n",
    "    def __init__(self, \n",
    "        original_np,\n",
    "        noisy_np,\n",
    "        noisy_torch,\n",
    "        learning_rate = 0.01,\n",
    "        show_every=20,\n",
    "        patience = 1000,\n",
    "        buffer_size = 100,\n",
    "        model=get_unet(),\n",
    "        weight_decay=5e-8,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # iterators\n",
    "        self.burnin_iter=0 # burn-in iteration for SGLD\n",
    "        self.show_every=show_every\n",
    "        self.num_iter=100 # max iterations\n",
    "\n",
    "        # backtracking\n",
    "        self.psrn_noisy_last=0\n",
    "        self.last_net = None\n",
    "        self.roll_back = True # To solve the oscillation of model training \n",
    "\n",
    "        # SGLD Output Accumulation\n",
    "        self.sgld_mean=0\n",
    "        self.sgld_mean_each=0\n",
    "        self.sgld_psnr_list = [] # psnr between sgld out and gt\n",
    "        self.MCMC_iter=50\n",
    "        self.param_noise_sigma=2\n",
    "\n",
    "        # tinker with image input\n",
    "        self.img_np = original_np           \n",
    "        self.img_noisy_np = noisy_np\n",
    "        self.img_noisy_torch = noisy_torch\n",
    "        \n",
    "        # network input\n",
    "        self.input_depth = 1\n",
    "        self.model = model.type(self.dtype) ### only for non NAS\n",
    "        self.net_input = get_noise(self.input_depth, 'noise', (img_np.shape[-2:][1], img_np.shape[-2:][0])).type(self.dtype).detach()\n",
    "        self.net_input_saved = self.net_input.detach().clone()\n",
    "        self.noise = self.net_input.detach().clone()\n",
    "        \n",
    "        # closure\n",
    "        self.reg_noise_std = tensor(1./30.)\n",
    "        self.criteria = torch.nn.MSELoss().type(dtype) # loss\n",
    "\n",
    "        # optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # burnin-end criteria\n",
    "        self.img_collection = []\n",
    "        self.variance_history = []\n",
    "        self.patience = patience\n",
    "        self.wait_count = 0\n",
    "        self.best_score = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        self.img_collection = []\n",
    "        self.burnin_over = False\n",
    "        self.buffer_size = buffer_size\n",
    "        self.cur_var = None\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        \"\"\"\n",
    "        We are doing a manual implementation of the SGLD optimizer\n",
    "        There is a SGLD optimizer that can be found here:\n",
    "            - https://pysgmcmc.readthedocs.io/en/pytorch/_modules/pysgmcmc/optimizers/sgld.html\n",
    "            - Implementing this would greatly affect the training step\n",
    "                - But could it work?? :`( I couldn't figure it out\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Trick this puppy into thinking we have a dataloader\n",
    "        It's a single image for deep image priors\n",
    "        So we just need to return a dataloader with a single image\n",
    "        \"\"\"\n",
    "        dataset = SingleImageDataset(self.img_np, self.num_iter)\n",
    "        return DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        \"\"\"\n",
    "        Move all tensors to the GPU to begin training\n",
    "        Initialize Iterators\n",
    "        Set Sail\n",
    "        \"\"\"\n",
    "        self.model.to(self.device)\n",
    "        self.net_input = self.net_input.to(self.device)\n",
    "        self.img_noisy_torch = self.img_noisy_torch.to(self.device)\n",
    "        self.reg_noise_std = self.reg_noise_std.to(self.device)\n",
    "\n",
    "        self.net_input_saved = self.net_input.clone().to(self.device)\n",
    "        self.noise = self.net_input.clone().to(self.device)\n",
    "        \n",
    "        # Initialize Iterations\n",
    "        self.i=0\n",
    "        self.sample_count=0\n",
    "        print('Starting optimization with SGLD')\n",
    "\n",
    "        # bon voyage\n",
    "        #                    *     .--.\n",
    "        #                         / /  `\n",
    "        #        +               | |\n",
    "        #               '         \\ \\__,\n",
    "        #           *          +   '--'  *\n",
    "        #               +   /\\\n",
    "        #  +              .'  '.   *\n",
    "        #         *      /======\\      +\n",
    "        #               ;:.  _   ;\n",
    "        #               |:. (_)  |\n",
    "        #               |:.  _   |\n",
    "        #     +         |:. (_)  |          *\n",
    "        #               ;:.      ;\n",
    "        #             .' \\:.    / `.\n",
    "        #            / .-'':._.'`-. \\\n",
    "        #            |/    /||\\    \\|\n",
    "        #          _..--\"\"\"````\"\"\"--.._\n",
    "        #    _.-'``                    ``'-._\n",
    "        #  -'                                '-\n",
    "\n",
    "\n",
    "    def forward(self, net_input_saved):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        occurs in the closure function in this implementation\n",
    "        \"\"\"\n",
    "        print(f\"made it to the start of the {self.i+1} forward start loop!\")\n",
    "        if self.reg_noise_std > 0:\n",
    "            self.net_input = self.net_input_saved + (self.noise.normal_() * self.reg_noise_std)\n",
    "            return self.model(self.net_input)\n",
    "        else:\n",
    "            return self.model(net_input_saved)\n",
    "\n",
    "    def update_burnin(self,out_np):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        check if we should end the burnin phase\n",
    "        \"\"\"\n",
    "        # update img collection\n",
    "        v_img_np = out_np.reshape(-1)\n",
    "        self.update_img_collection(v_img_np)\n",
    "        img_collection = self.get_img_collection()\n",
    "\n",
    "        if len(img_collection) >= self.buffer_size:\n",
    "            # update variance and var history\n",
    "            ave_img = np.mean(img_collection, axis=0)\n",
    "            variance = [self.MSE(ave_img, tmp) for tmp in img_collection]\n",
    "            self.cur_var = np.mean(variance)\n",
    "            self.variance_history.append(self.cur_var)\n",
    "            self.check_stop(self.cur_var, self.i)\n",
    "    \n",
    "    def backtracking(self, psrn_noisy, total_loss):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        backtracking to prevent oscillation if the PSNR is fluctuating\n",
    "        \"\"\"\n",
    "        if self.roll_back and self.i % self.show_every:\n",
    "            if psrn_noisy - self.psrn_noisy_last < -5: \n",
    "                print('Falling back to previous checkpoint.')\n",
    "                for new_param, net_param in zip(self.last_net, self.model.parameters()):\n",
    "                    net_param.detach().copy_(new_param.cuda())\n",
    "                return total_loss*0\n",
    "            else:\n",
    "                self.last_net = [x.detach().cpu() for x in self.model.parameters()]\n",
    "                self.psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    def closure_sgld(self):\n",
    "        print(f\"made it to the start of the {self.i+1} closure start loop!\")\n",
    "        out = self.forward(self.net_input)\n",
    "\n",
    "        # compute loss\n",
    "        total_loss = self.criteria(out, self.img_noisy_torch)\n",
    "        total_loss.backward()\n",
    "        out_np = out.detach().cpu().numpy()[0]\n",
    "\n",
    "        # compute PSNR\n",
    "        psrn_noisy = compare_psnr(self.img_noisy_np, out.detach().cpu().numpy()[0])\n",
    "        psrn_gt    = compare_psnr(self.img_np, out_np)\n",
    "        self.sgld_psnr_list.append(psrn_gt)\n",
    "\n",
    "        # early burn in termination criteria\n",
    "        if not self.burnin_over:\n",
    "            self.update_burnin(out_np)\n",
    "\n",
    "        # backtracking \n",
    "        self.backtracking(psrn_noisy, total_loss)\n",
    "\n",
    "        ##########################################\n",
    "        ### Logging and SGLD mean collection #####\n",
    "        ##########################################\n",
    "        \n",
    "        if self.burnin_over and np.mod(self.i, self.MCMC_iter) == 0:\n",
    "            self.sgld_mean += out_np\n",
    "            self.sample_count += 1.\n",
    "            # sgld_psnr = compare_psnr(self.img_np, self.sgld_mean / self.sample_count)\n",
    "            # self.log({'loss': total_loss, 'psnr_gt': psrn_gt, 'psnr_sgld': sgld_psnr})\n",
    "            # nni.report_intermediate_result({'loss': total_loss, 'psnr_gt': psrn_gt, 'psnr_sgld': sgld_psnr})\n",
    "\n",
    "        if self.burnin_over:\n",
    "            self.burnin_iter+=1\n",
    "            self.sgld_mean_each += out_np\n",
    "\n",
    "        # elif self.cur_var is not None and not self.burnin_over:\n",
    "        #     self.log({'loss': total_loss, 'psnr_gt': psrn_gt, 'var': self.cur_var})\n",
    "        #     nni.report_intermediate_result({'loss': total_loss, 'psnr_gt': psrn_gt, 'var': self.cur_var})\n",
    "\n",
    "        # else:\n",
    "        #     self.log({'loss': total_loss, 'psnr_gt': psrn_gt})\n",
    "        #     nni.report_intermediate_result({'loss': total_loss, 'psnr_gt': psrn_gt})\n",
    "\n",
    "        self.i += 1\n",
    "        return total_loss\n",
    "\n",
    "    def add_noise(self, net):\n",
    "        \"\"\"\n",
    "        Add noise to the network parameters\n",
    "        This is the critical part of SGLD\n",
    "        \"\"\"\n",
    "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "            noise = torch.randn(n.size())*self.param_noise_sigma*self.learning_rate\n",
    "            noise = noise.type(dtype)\n",
    "            n.data = n.data + noise\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        \"\"\"\n",
    "        Oh the places you'll go\n",
    "        ---> Straight to error city calling this add_noise in the training step\n",
    "        ---> Consider using the on_train_batch_end hook? (each batch is only one iteration)\n",
    "        \"\"\"\n",
    "        print(f\"made it to the start of the {self.i+1} training start loop!\")\n",
    "        optimizer = self.optimizers()\n",
    "        print(f\"made it past the optimizer!\")\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"made it past the zero grad!\")\n",
    "        loss = self.closure_sgld()\n",
    "        print(f\"made it past the closure!\")\n",
    "        optimizer.step()\n",
    "        print(f\"made it past the step!\")\n",
    "        self.add_noise(self.model)\n",
    "        print(f\"made it past the add noise!\")\n",
    "        if self.i % 20 == 0:\n",
    "            self.log('psnr',self.sgld_psnr_list[-1])\n",
    "            nni.report_intermediate_result(self.sgld_psnr_list[-1])\n",
    "        print(f\"made it to the end of the {self.i+1} training start loop!\")\n",
    "        return loss\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        \"\"\"\n",
    "        May all your dreams come true\n",
    "        \"\"\"\n",
    "        nni.report_final_result(self.sgld_psnr_list[-1])\n",
    "\n",
    "    def check_stop(self, current, cur_epoch):\n",
    "        \"\"\"\n",
    "        using an early stopper technique to determine when to end the burn in phase for SGLD\n",
    "        https://arxiv.org/pdf/2112.06074.pdf\n",
    "        https://github.com/sun-umn/Early_Stopping_for_DIP/blob/main/ES_WMV.ipynb\n",
    "        \"\"\"\n",
    "        if current < self.best_score:\n",
    "            self.best_score = current\n",
    "            self.best_epoch = cur_epoch\n",
    "            self.wait_count = 0\n",
    "            self.burnin_over = False\n",
    "        else:\n",
    "            self.wait_count += 1\n",
    "            self.burnin_over = self.wait_count >= self.patience\n",
    "        if self.burnin_over:\n",
    "            print(f'\\n\\nBurn-in completed at iter {self.i}; \\nStarting SGLD Mean sampling;\\n\\n')\n",
    "            self.show_every = self.MCMC_iter\n",
    "\n",
    "    def update_img_collection(self, cur_img):\n",
    "        self.img_collection.append(cur_img)\n",
    "        if len(self.img_collection) > self.buffer_size:\n",
    "            self.img_collection.pop(0)\n",
    "\n",
    "    def get_img_collection(self):\n",
    "        return self.img_collection\n",
    "\n",
    "    def MSE(self, x1, x2):\n",
    "        return ((x1 - x2) ** 2).sum() / x1.size\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'patience': 1000,\n",
    "        'buffer_size': 100,\n",
    "        'weight_decay': 5e-8, # this is proportionate to a 1024x1024 image\n",
    "        }\n",
    "\n",
    "optimized_params = nni.get_next_parameter()\n",
    "params.update(optimized_params)\n",
    "print(params)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# check if CUDA is available\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor \n",
    "\n",
    "# choose iterations\n",
    "num_iter = 100 # max iterations\n",
    "\n",
    "# get image\n",
    "# generate phantom stored in subfolder of parent directory\n",
    "resolution = 6\n",
    "max_depth = resolution - 1\n",
    "phantom = generate_phantom(resolution=resolution)\n",
    "raw_img_np = phantom.copy() # 1x64x64 np array    \n",
    "img_np = raw_img_np.copy() # 1x64x64 np array\n",
    "sigma=25/255\n",
    "# sigma = .05\n",
    "img_noisy_np = np.clip(img_np + np.random.normal(scale=sigma, size=img_np.shape), 0, 1).astype(np.float32)\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
    "\n",
    "# reference model \n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# Create the lightning module\n",
    "module = SGLD_HPO(\n",
    "        original_np=img_np,\n",
    "        noisy_np=img_noisy_np,\n",
    "        noisy_torch=img_noisy_torch,\n",
    "        learning_rate = params['learning_rate'],\n",
    "        patience=params['patience'],\n",
    "        buffer_size=params['buffer_size'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        )\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = Trainer(\n",
    "            max_epochs=num_iter,\n",
    "            fast_dev_run=False,\n",
    "            gpus=1,\n",
    "            accelerator='gpu',\n",
    "            enable_checkpointing=False,\n",
    "            log_every_n_steps=20,\n",
    "            )\n",
    "\n",
    "            \n",
    "# Create the lighting object for evaluator\n",
    "train_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1, num_workers=0)\n",
    "val_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1, num_workers=0)\n",
    "\n",
    "lightning = Lightning(lightning_module=module, trainer=trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose iterations\n",
    "num_iter = 100 # max iterations\n",
    "\n",
    "# get image\n",
    "# generate phantom stored in subfolder of parent directory\n",
    "resolution = 6\n",
    "max_depth = resolution - 1\n",
    "phantom = generate_phantom(resolution=resolution)\n",
    "raw_img_np = phantom.copy() # 1x64x64 np array    \n",
    "img_np = raw_img_np.copy() # 1x64x64 np array\n",
    "sigma=25/255\n",
    "# sigma = .05\n",
    "img_noisy_np = np.clip(img_np + np.random.normal(scale=sigma, size=img_np.shape), 0, 1).astype(np.float32)\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
    "\n",
    "# reference model \n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# Create the lightning module\n",
    "module = SGLD_HPO(\n",
    "        original_np=img_np,\n",
    "        noisy_np=img_noisy_np,\n",
    "        noisy_torch=img_noisy_torch,\n",
    "        learning_rate = params['learning_rate'],\n",
    "        patience=params['patience'],\n",
    "        buffer_size=params['buffer_size'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        )\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = Trainer(\n",
    "            max_epochs=num_iter,\n",
    "            fast_dev_run=False,\n",
    "            gpus=1,\n",
    "            accelerator='gpu',\n",
    "            enable_checkpointing=False,\n",
    "            log_every_n_steps=20,\n",
    "            )\n",
    "\n",
    "            \n",
    "# Create the lighting object for evaluator\n",
    "train_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1, num_workers=0)\n",
    "val_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1, num_workers=0)\n",
    "\n",
    "lightning = Lightning(lightning_module=module, trainer=trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "lightning.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas-test-OHy8kATa-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
