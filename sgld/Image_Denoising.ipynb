{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from models import *\n",
    "import torch\n",
    "import torch.optim\n",
    "import time\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from utils.denoising_utils import *\n",
    "import _pickle as cPickle\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# display images\n",
    "def np_plot(np_matrix, title):\n",
    "    plt.clf()\n",
    "    fig = plt.imshow(np_matrix.transpose(1, 2, 0), interpolation = 'nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.05) \n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/denoising/Dataset/image_Peppers512rgb.png'\n",
    "imsize =-1\n",
    "sigma = 25/255.\n",
    "img_pil = crop_image(get_image(fname, imsize)[0], d=32)\n",
    "img_np = pil_to_np(img_pil)                \n",
    "img_noisy_pil, img_noisy_np = get_noisy_image(img_np, sigma)\n",
    "np_plot(img_np, 'Natural image')\n",
    "np_plot(img_noisy_np, 'Noisy image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT = 'noise'\n",
    "pad = 'reflection'\n",
    "OPT_OVER = 'net' # optimize over the net parameters only\n",
    "reg_noise_std = 1./30.\n",
    "learning_rate = LR = 0.01\n",
    "exp_weight=0.99\n",
    "input_depth = 32 \n",
    "roll_back = True # to prevent numerical issues\n",
    "num_iter = 20000 # max iterations\n",
    "burnin_iter = 7000 # burn-in iteration for SGLD\n",
    "weight_decay = 5e-8\n",
    "show_every =  500\n",
    "mse = torch.nn.MSELoss().type(dtype) # loss\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD variant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) SGD  and (2) SGD + moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = 500 # checkout the prediction at 500-th iteration\n",
    "\n",
    "# skip net\n",
    "net = get_net(input_depth, 'skip', pad,\n",
    "            skip_n33d=128, \n",
    "            skip_n33u=128,\n",
    "            skip_n11=4,\n",
    "            num_scales=5,\n",
    "            upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "# Optimize\n",
    "net_input = get_noise(input_depth, INPUT, (img_pil.size[1], img_pil.size[0])).type(dtype).detach()\n",
    "out_avg = None\n",
    "last_net = None\n",
    "psrn_noisy_last = 0\n",
    "i = 0\n",
    "\n",
    "# save tmp PSNR for different learning strategies\n",
    "sgd_noise_psnr_list = [] # psnr between out and noise image\n",
    "sgd_psnr_list = [] # psnr between sgd out and gt\n",
    "sgd_expm_psnr_list = [] # psnr between exp avg and gt\n",
    "sgd_mse_list = [] # Mean squared error\n",
    "\n",
    "sgd_out_500 = None\n",
    "sgd_expm_out_500 = None\n",
    "\n",
    "def closure():\n",
    "\n",
    "    global i, out_avg, psrn_noisy_last, last_net, net_input, sgd_out_500, sgd_expm_out_500\n",
    "    out = net(net_input)\n",
    "    if out_avg is None:\n",
    "        out_avg = out.detach()\n",
    "    else:\n",
    "        out_avg = out_avg * exp_weight + out.detach() * (1 - exp_weight)\n",
    "    total_loss = mse(out, img_noisy_torch)\n",
    "    total_loss.backward()\n",
    "\n",
    "    out_np = out.detach().cpu().numpy()[0]\n",
    "    out_avg_np = out_avg.detach().cpu().numpy()[0]\n",
    "\n",
    "    psrn_noisy = compare_psnr(img_noisy_np, out_np)\n",
    "    psrn_gt    = compare_psnr(img_np, out_np)\n",
    "    psrn_gt_sm = compare_psnr(img_np, out_avg_np)\n",
    "\n",
    "    sgd_noise_psnr_list.append(psrn_noisy)\n",
    "    sgd_psnr_list.append(psrn_gt)\n",
    "    sgd_expm_psnr_list.append(psrn_gt_sm)\n",
    "\n",
    "    if i % show_every == 0:\n",
    "        np_plot(out.detach().cpu().numpy()[0], 'Iter: %d; gt %.2f; sm %.2f' % (i, psrn_gt, psrn_gt_sm))\n",
    "        \n",
    "    if i == check_point - 1:\n",
    "        sgd_out_500 =  out_np\n",
    "        sgd_expm_out_500 =  out_avg_np\n",
    "\n",
    "    # Backtracking (TODO: check this later)\n",
    "    if roll_back and i % show_every:\n",
    "        if psrn_noisy - psrn_noisy_last < -5: \n",
    "            print('Falling back to previous checkpoint.')\n",
    "            for new_param, net_param in zip(last_net, net.parameters()):\n",
    "                net_param.detach().copy_(new_param.cuda())\n",
    "            return total_loss*0\n",
    "        else:\n",
    "            last_net = [x.detach().cpu() for x in net.parameters()]\n",
    "            psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    i += 1\n",
    "    return total_loss\n",
    "\n",
    "## Optimizing \n",
    "print('Starting optimization with ADAM')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "for j in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    closure()\n",
    "    optimizer.step()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) SGD + weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point = 500 # checkout the prediction at 500-th iteration\n",
    "\n",
    "# skip net\n",
    "net = get_net(input_depth, 'skip', pad,\n",
    "            skip_n33d=128, \n",
    "            skip_n33u=128,\n",
    "            skip_n11=4,\n",
    "            num_scales=5,\n",
    "            upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "# Optimize\n",
    "net_input = get_noise(input_depth, INPUT, (img_pil.size[1], img_pil.size[0])).type(dtype).detach()\n",
    "out_avg = None\n",
    "last_net = None\n",
    "psrn_noisy_last = 0\n",
    "i = 0\n",
    "\n",
    "sgd_wd_psnr_list = [] # psnr between sgd out and gt\n",
    "\n",
    "def closure():\n",
    "\n",
    "    global i, out_avg, psrn_noisy_last, last_net, net_input\n",
    "    out = net(net_input)\n",
    "    total_loss = mse(out, img_noisy_torch)\n",
    "    total_loss.backward()\n",
    "\n",
    "    out_np = out.detach().cpu().numpy()[0]\n",
    "    psrn_noisy = compare_psnr(img_noisy_np, out_np)\n",
    "    psrn_gt    = compare_psnr(img_np, out_np)\n",
    "    sgd_wd_psnr_list.append(psrn_gt)\n",
    "\n",
    "    if i % show_every == 0:\n",
    "        np_plot(out.detach().cpu().numpy()[0], 'Iter: %d; gt %.2f' % (i, psrn_gt))\n",
    "        \n",
    "    # Backtracking\n",
    "    if roll_back and i % show_every:\n",
    "        if psrn_noisy - psrn_noisy_last < -5: \n",
    "            print('Falling back to previous checkpoint.')\n",
    "            for new_param, net_param in zip(last_net, net.parameters()):\n",
    "                net_param.detach().copy_(new_param.cuda())\n",
    "            return total_loss*0\n",
    "        else:\n",
    "            last_net = [x.detach().cpu() for x in net.parameters()]\n",
    "            psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    i += 1\n",
    "    return total_loss\n",
    "\n",
    "## Optimizing \n",
    "print('Starting optimization with ADAM')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay = weight_decay) ## SGD + weight decay\n",
    "for j in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    closure()\n",
    "    optimizer.step()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) SGD + input noise  and (5) SGD + input noise + moiving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_encoder_kernel_sigma():\n",
    "    input_sigma_list = [0, 20, 60, 120]\n",
    "    num_layers = 3\n",
    "    color = ['r', 'b', 'g', 'y']\n",
    "    for idx, in_sigma in enumerate(input_sigma_list):\n",
    "        samples = prior_sampling(in_sigma, net_width, num_layers, num_samples = 50000, out_size = 2048, cnn = 'auto_encoder')\n",
    "        mean, kd = compute_mean_cov_from_samples(samples, margin = 1000)        \n",
    "        NN_cos_ang = kd/kd[0]\n",
    "        x = np.arange(-len(mean)+1, len(mean))\n",
    "        nn_cos_all = np.concatenate((np.flip(NN_cos_ang), NN_cos_ang[1:]))\n",
    "        plt.plot(x, nn_cos_all, color[idx], label = r'$\\sigma=%d$'% in_sigma)\n",
    "        plt.legend(loc='lower center', bbox_to_anchor=(0.5, 1.00001), ncol=4, prop={'size':9})\n",
    "    plt.xlabel(r'$t_1 - t_2$' + r'  ($d=3$)')\n",
    "    plt.ylabel(r'$\\cos \\theta_{t_1, t_2}$')\n",
    "    plt.show()check_point = 1800\n",
    "\n",
    "net = get_net(input_depth, 'skip', pad,\n",
    "            skip_n33d=128, \n",
    "            skip_n33u=128,\n",
    "            skip_n11=4,\n",
    "            num_scales=5,\n",
    "            upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "## Optimize\n",
    "net_input = get_noise(input_depth, INPUT, (img_pil.size[1], img_pil.size[0])).type(dtype).detach()\n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "out_avg = None\n",
    "last_net = None\n",
    "psrn_noisy_last = 0\n",
    "show_every = 100\n",
    "i = 0\n",
    "\n",
    "# save tmp PSNR for different learning strategies\n",
    "fancy_sgd_noise_psnr_list = [] # psnr between out and noise image\n",
    "fancy_sgd_psnr_list = [] # psnr between sgd out and gt\n",
    "fancy_sgd_expm_psnr_list = [] # psnr between exp avg and gt\n",
    "fancy_sgd_out_1800 = None\n",
    "fancy_sgd_expm_out_1800 = None\n",
    "\n",
    "def closure():\n",
    "\n",
    "    global i, out_avg, psrn_noisy_last, last_net, net_input, fancy_sgd_out_1800, fancy_sgd_expm_out_1800\n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "    out = net(net_input)\n",
    "    if out_avg is None:\n",
    "        out_avg = out.detach()\n",
    "    else:\n",
    "        out_avg = out_avg * exp_weight + out.detach() * (1 - exp_weight)\n",
    "    total_loss = mse(out, img_noisy_torch)\n",
    "    total_loss.backward()\n",
    "\n",
    "    out_np = out.detach().cpu().numpy()[0]\n",
    "    out_avg_np = out_avg.detach().cpu().numpy()[0]\n",
    "\n",
    "    psrn_noisy = compare_psnr(img_noisy_np, out_np)\n",
    "    psrn_gt    = compare_psnr(img_np, out_np)\n",
    "    psrn_gt_sm = compare_psnr(img_np, out_avg_np)\n",
    "\n",
    "    fancy_sgd_noise_psnr_list.append(psrn_noisy)\n",
    "    fancy_sgd_psnr_list.append(psrn_gt)\n",
    "    fancy_sgd_expm_psnr_list.append(psrn_gt_sm)\n",
    "\n",
    "    if i == check_point - 1:\n",
    "        fancy_sgd_out_1800 =  out_np\n",
    "        fancy_sgd_expm_out_1800 =  out_avg_np\n",
    "\n",
    "    if i % show_every == 0:\n",
    "        np_plot(out.detach().cpu().numpy()[0], 'Iter: %d; gt %.2f; sm %.2f' % (i, psrn_gt, psrn_gt_sm))\n",
    "\n",
    "    # Backtracking\n",
    "    if roll_back and i % show_every:\n",
    "        if psrn_noisy - psrn_noisy_last < -5: \n",
    "            print('Falling back to previous checkpoint.')\n",
    "            for new_param, net_param in zip(last_net, net.parameters()):\n",
    "                net_param.detach().copy_(new_param.cuda())\n",
    "            return total_loss*0\n",
    "        else:\n",
    "            last_net = [x.detach().cpu() for x in net.parameters()]\n",
    "            psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    i += 1\n",
    "    return total_loss\n",
    "\n",
    "## Optimizing \n",
    "print('Starting optimization with ADAM')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "for j in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    closure()\n",
    "    optimizer.step()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGLD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgld_psnr_list = [] # psnr between sgld out and gt\n",
    "sgld_mean = 0\n",
    "roll_back = True # To solve the oscillation of model training \n",
    "last_net = None\n",
    "psrn_noisy_last = 0\n",
    "MCMC_iter = 50\n",
    "param_noise_sigma = 2\n",
    "\n",
    "sgld_mean_each = 0\n",
    "sgld_psnr_mean_list = [] # record the PSNR of avg after burn-in\n",
    "\n",
    "# def cache_sample(iteration, data):\n",
    "#     with open(dirname_samples + '/%d'%iteration, 'wb') as f:\n",
    "#         cPickle.dump(data, f)\n",
    "\n",
    "## SGLD\n",
    "def add_noise(model):\n",
    "    for n in [x for x in model.parameters() if len(x.size()) == 4]:\n",
    "        noise = torch.randn(n.size())*param_noise_sigma*learning_rate\n",
    "        noise = noise.type(dtype)\n",
    "        n.data = n.data + noise\n",
    "\n",
    "net2 = get_net(input_depth, 'skip', pad,\n",
    "            skip_n33d=128, \n",
    "            skip_n33u=128,\n",
    "            skip_n11=4,\n",
    "            num_scales=5,\n",
    "            upsample_mode='bilinear').type(dtype)\n",
    "\n",
    "## Input random noise\n",
    "net_input = get_noise(input_depth, INPUT, (img_pil.size[1], img_pil.size[0])).type(dtype).detach()\n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "i = 0\n",
    "\n",
    "sample_count = 0\n",
    "\n",
    "def closure_sgld():\n",
    "    global i, net_input, sgld_mean, sample_count, psrn_noisy_last, last_net, sgld_mean_each\n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "    out = net2(net_input)\n",
    "    total_loss = mse(out, img_noisy_torch)\n",
    "    total_loss.backward()\n",
    "    out_np = out.detach().cpu().numpy()[0]\n",
    "\n",
    "    psrn_noisy = compare_psnr(img_noisy_np, out.detach().cpu().numpy()[0])\n",
    "    psrn_gt    = compare_psnr(img_np, out_np)\n",
    "\n",
    "    sgld_psnr_list.append(psrn_gt)\n",
    "\n",
    "    # Backtracking\n",
    "    if roll_back and i % show_every:\n",
    "        if psrn_noisy - psrn_noisy_last < -5: \n",
    "            print('Falling back to previous checkpoint.')\n",
    "            for new_param, net_param in zip(last_net, net2.parameters()):\n",
    "                net_param.detach().copy_(new_param.cuda())\n",
    "            return total_loss*0\n",
    "        else:\n",
    "            last_net = [x.detach().cpu() for x in net2.parameters()]\n",
    "            psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    if i % show_every == 0:\n",
    "        np_plot(out.detach().cpu().numpy()[0], 'Iter: %d; gt %.2f' % (i, psrn_gt))\n",
    "    \n",
    "    if i > burnin_iter and np.mod(i, MCMC_iter) == 0:\n",
    "        sgld_mean += out_np\n",
    "        sample_count += 1.\n",
    "\n",
    "    if i > burnin_iter:\n",
    "        sgld_mean_each += out_np\n",
    "        sgld_mean_tmp = sgld_mean_each / (i - burnin_iter)\n",
    "        sgld_mean_psnr_each = compare_psnr(img_np, sgld_mean_tmp)\n",
    "        sgld_psnr_mean_list.append(sgld_mean_psnr_each) # record the PSNR of avg after burn-in\n",
    "        print('Iter: %d; psnr_gt %.2f; psnr_sgld %.2f' % (i, psrn_gt, sgld_mean_psnr_each))\n",
    "    else:\n",
    "        print('Iter: %d; psnr_gt %.2f; loss %.5f' % (i, psrn_gt, total_loss))\n",
    "    \n",
    "    if i == burnin_iter:\n",
    "        print('Burn-in done, start sampling')\n",
    "\n",
    "    i += 1\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "  ## Optimizing \n",
    "print('Starting optimization with SGLD')\n",
    "optimizer = torch.optim.Adam(net2.parameters(), lr=LR, weight_decay = weight_decay)\n",
    "for j in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    closure_sgld()\n",
    "    optimizer.step()\n",
    "    add_noise(net2)\n",
    "\n",
    "sgld_mean = sgld_mean / sample_count\n",
    "sgld_mean_psnr = compare_psnr(img_np, sgld_mean)\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(img_np.squeeze(), cmap='gray')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(sgld_mean.squeeze(), cmap='gray')\n",
    "ax[1].set_title(\"Denoised Image\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(img_noisy_torch.detach().cpu().squeeze().numpy(), cmap='gray')\n",
    "ax[2].set_title(\"Noisy Image\")\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the shape of img_np\n",
    "print('img_np shape: %f' % img_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SGD variants and SGLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PSNR curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(9,5))    \n",
    "num_iter = len(sgd_psnr_list)\n",
    "num_iter = 20000\n",
    "\n",
    "\n",
    "sgd_psnr_list = np.array(sgd_psnr_list)\n",
    "sgd_expm_psnr_list = np.array(sgd_expm_psnr_list)\n",
    "sgd_wd_psnr_list = np.array(sgd_wd_psnr_list)\n",
    "fancy_sgd_psnr_list = np.array(fancy_sgd_psnr_list)\n",
    "fancy_sgd_expm_psnr_list = np.array(fancy_sgd_expm_psnr_list)\n",
    "sgld_psnr_mean_list = np.array(sgld_psnr_mean_list)\n",
    "sgld_psnr_list = np.array(sgld_psnr_list)\n",
    "\n",
    "\n",
    "# SGD\n",
    "x_iters = np.arange(0, num_iter, 50)\n",
    "plt.plot(x_iters, sgd_psnr_list[x_iters], 'r', label = 'SGD')\n",
    "plt.plot(x_iters, sgd_expm_psnr_list[x_iters], 'r--', label = 'SGD+Avg')\n",
    "\n",
    "# with weight decay\n",
    "plt.plot(x_iters, sgd_wd_psnr_list[x_iters], color = 'k', label = 'SGD+WD')\n",
    "\n",
    "# SGD + Input\n",
    "plt.plot(x_iters, fancy_sgd_psnr_list[x_iters], 'y', label = 'SGD+Input')\n",
    "plt.plot(x_iters, fancy_sgd_expm_psnr_list[x_iters],'y--', label = 'SGD+Input+Avg')\n",
    "\n",
    "# SGLD\n",
    "burnin_iter = 7000\n",
    "x_iters_after_burnin = np.arange(len(sgld_psnr_mean_list)) + burnin_iter \n",
    "plt.plot(x_iters, sgld_psnr_list[x_iters], 'b', label = 'SGLD')\n",
    "plt.plot(np.arange(burnin_iter, num_iter-1), sgld_psnr_mean_list, 'b--', label = 'SGLD Avg w.r.t. Iters')\n",
    "plt.plot([0, 5000, 10000, 15000, 20000], [sgld_mean_psnr] * 5, '--bo', label = 'SGLD Avg')\n",
    "\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, 1.0001), ncol = 4, prop={'size':10.5})\n",
    "plt.xlabel('Iteration')\n",
    "plt.xticks([0, 5000, 10000, 15000, 20000], ('0', '5K', '10K', '15K', '20K'))\n",
    "plt.ylim(20, 31)\n",
    "plt.ylabel('PSNR (dB)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
