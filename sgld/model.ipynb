{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/joe/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'patience': 1000, 'buffer_size': 100, 'weight_decay': 5e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/joe/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "TPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 368\u001b[0m\n\u001b[1;32m    357\u001b[0m module \u001b[39m=\u001b[39m SGLD_HPO(\n\u001b[1;32m    358\u001b[0m         original_np\u001b[39m=\u001b[39mimg_np,\n\u001b[1;32m    359\u001b[0m         noisy_np\u001b[39m=\u001b[39mimg_noisy_np,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m         weight_decay\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    365\u001b[0m         )\n\u001b[1;32m    367\u001b[0m \u001b[39m# Create a PyTorch Lightning trainer\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    369\u001b[0m             max_epochs\u001b[39m=\u001b[39;49mnum_iter,\n\u001b[1;32m    370\u001b[0m             fast_dev_run\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    371\u001b[0m             \u001b[39m# gpus=1,\u001b[39;49;00m\n\u001b[1;32m    372\u001b[0m             accelerator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    373\u001b[0m             devices\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    374\u001b[0m             checkpoint_callback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    375\u001b[0m             )\n\u001b[1;32m    377\u001b[0m \u001b[39m# Initialize ModelCheckpoint callback\u001b[39;00m\n\u001b[1;32m    378\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m    379\u001b[0m     dirpath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{lightning_logs}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{logger_name}\u001b[39;00m\u001b[39m/version_\u001b[39m\u001b[39m{version}\u001b[39;00m\u001b[39m/checkpoints/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    380\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{epoch}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{step}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    381\u001b[0m     every_n_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m    382\u001b[0m     save_top_k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/nni/common/serializer.py:508\u001b[0m, in \u001b[0;36m_trace_cls.<locals>.wrapper.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m args, kwargs \u001b[39m=\u001b[39m _formulate_arguments(base\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m, args, kwargs, kw_only, is_class_init\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    506\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[39m# calling serializable object init to initialize the full object\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(symbol\u001b[39m=\u001b[39;49mbase, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs, call_super\u001b[39m=\u001b[39;49mcall_super)\n\u001b[1;32m    509\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRecursionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    510\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mRecursion error detected in initialization of wrapped object. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mDid you use `super(MyClass, self).__init__()` rather than `super().__init__()`? \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[39mRuntimeWarning\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/nni/common/serializer.py:133\u001b[0m, in \u001b[0;36mSerializableObject.__init__\u001b[0;34m(self, symbol, args, kwargs, call_super)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_nni_call_super\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m call_super\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m call_super:\n\u001b[1;32m    132\u001b[0m     \u001b[39m# call super means that the serializable object is by itself an object of the target class\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    134\u001b[0m         \u001b[39m*\u001b[39;49m[_argument_processor(arg) \u001b[39mfor\u001b[39;49;00m arg \u001b[39min\u001b[39;49;00m args],\n\u001b[1;32m    135\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{kw: _argument_processor(arg) \u001b[39mfor\u001b[39;49;00m kw, arg \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems()}\n\u001b[1;32m    136\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/pytorch_lightning/utilities/argparse.py:339\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    338\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:483\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> 483\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[1;32m    484\u001b[0m     num_processes\u001b[39m=\u001b[39;49mnum_processes,\n\u001b[1;32m    485\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    486\u001b[0m     tpu_cores\u001b[39m=\u001b[39;49mtpu_cores,\n\u001b[1;32m    487\u001b[0m     ipus\u001b[39m=\u001b[39;49mipus,\n\u001b[1;32m    488\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    489\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    490\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[1;32m    491\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m    492\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    493\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m    494\u001b[0m     replace_sampler_ddp\u001b[39m=\u001b[39;49mreplace_sampler_ddp,\n\u001b[1;32m    495\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    496\u001b[0m     auto_select_gpus\u001b[39m=\u001b[39;49mauto_select_gpus,\n\u001b[1;32m    497\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    498\u001b[0m     amp_type\u001b[39m=\u001b[39;49mamp_backend,\n\u001b[1;32m    499\u001b[0m     amp_level\u001b[39m=\u001b[39;49mamp_level,\n\u001b[1;32m    500\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    501\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m LoggerConnector(\u001b[39mself\u001b[39m, log_gpu_memory)\n\u001b[1;32m    503\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:196\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_accelerator()\n\u001b[0;32m--> 196\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_parallel_devices_and_init_accelerator()\n\u001b[1;32m    198\u001b[0m \u001b[39m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_and_init_cluster_environment()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/nas-test-OHy8kATa-py3.8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:514\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    511\u001b[0m     available_accelerator \u001b[39m=\u001b[39m [\n\u001b[1;32m    512\u001b[0m         acc_str \u001b[39mfor\u001b[39;00m acc_str \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_types \u001b[39mif\u001b[39;00m AcceleratorRegistry\u001b[39m.\u001b[39mget(acc_str)\u001b[39m.\u001b[39mis_available()\n\u001b[1;32m    513\u001b[0m     ]\n\u001b[0;32m--> 514\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    515\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m can not run on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer`: \u001b[39m\u001b[39m{\u001b[39;00mavailable_accelerator\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_devices_flag_if_auto_passed()\n\u001b[1;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_devices_flag \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: TPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "# this is the HPO Search Space\n",
    "\n",
    "import nni\n",
    "from torch import nn, tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from models import *\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from utils.denoising_utils import *\n",
    "\n",
    "from phantom import generate_phantom\n",
    "\n",
    "from nni.retiarii.evaluator.pytorch import Lightning, Trainer, LightningModule\n",
    "from nni.retiarii.evaluator.pytorch.lightning import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "\n",
    "def get_unet():\n",
    "    return torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "\n",
    "# SGLD Pytorch Lightning Module\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, image, num_iter):\n",
    "        self.image = image\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_iter\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Always return the same image (and maybe a noise tensor or other information if necessary??)\n",
    "        return self.image\n",
    "\n",
    "class SGLD_HPO(LightningModule):\n",
    "    def __init__(self, \n",
    "        original_np,\n",
    "        noisy_np,\n",
    "        noisy_torch,\n",
    "        learning_rate = 0.01,\n",
    "        show_every=20,\n",
    "        patience = 1000,\n",
    "        buffer_size = 100,\n",
    "        model=get_unet(),\n",
    "        weight_decay=5e-8,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # iterators\n",
    "        self.burnin_iter=0 # burn-in iteration for SGLD\n",
    "        self.show_every=show_every\n",
    "        self.num_iter=20000\n",
    "\n",
    "        # backtracking\n",
    "        self.psrn_noisy_last=0\n",
    "        self.last_net = None\n",
    "        self.roll_back = True # To solve the oscillation of model training \n",
    "\n",
    "        # SGLD Output Accumulation\n",
    "        self.sgld_mean=0\n",
    "        self.sgld_mean_each=0\n",
    "        self.sgld_psnr_list = [] # psnr between sgld out and gt\n",
    "        self.MCMC_iter=50\n",
    "        self.param_noise_sigma=2\n",
    "\n",
    "        # tinker with image input\n",
    "        self.img_np = original_np           \n",
    "        self.img_noisy_np = noisy_np\n",
    "        self.img_noisy_torch = noisy_torch\n",
    "        \n",
    "        # network input\n",
    "        self.input_depth = 1\n",
    "        self.model = model.type(self.dtype)\n",
    "        self.net_input = get_noise(self.input_depth, 'noise', (img_np.shape[-2:][1], img_np.shape[-2:][0])).type(self.dtype).detach()\n",
    "        self.net_input_saved = self.net_input.detach().clone()\n",
    "        self.noise = self.net_input.detach().clone()\n",
    "        \n",
    "        # closure\n",
    "        self.reg_noise_std = tensor(1./30.)\n",
    "        self.criteria = torch.nn.MSELoss().type(dtype) # loss\n",
    "\n",
    "        # optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # burnin-end criteria\n",
    "        self.img_collection = []\n",
    "        self.variance_history = []\n",
    "        self.patience = patience\n",
    "        self.wait_count = 0\n",
    "        self.best_score = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        self.img_collection = []\n",
    "        self.burnin_over = False\n",
    "        self.buffer_size = buffer_size\n",
    "        self.cur_var = None\n",
    "\n",
    "    def configure_optimizers(self) -> Optimizer:\n",
    "        \"\"\"\n",
    "        We are doing a manual implementation of the SGLD optimizer\n",
    "        There is a SGLD optimizer that can be found here:\n",
    "            - https://pysgmcmc.readthedocs.io/en/pytorch/_modules/pysgmcmc/optimizers/sgld.html\n",
    "            - Implementing this would greatly affect the training step\n",
    "                - But could it work?? :`( I couldn't figure it out\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Trick this puppy into thinking we have a dataloader\n",
    "        It's a single image for deep image priors\n",
    "        So we just need to return a dataloader with a single image\n",
    "        \"\"\"\n",
    "        dataset = SingleImageDataset(self.img_np, self.num_iter)\n",
    "        return DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        \"\"\"\n",
    "        Move all tensors to the GPU to begin training\n",
    "        Initialize Iterators\n",
    "        Set Sail\n",
    "        \"\"\"\n",
    "        self.model.to(self.device)\n",
    "        self.net_input = self.net_input.to(self.device)\n",
    "        self.img_noisy_torch = self.img_noisy_torch.to(self.device)\n",
    "        self.reg_noise_std = self.reg_noise_std.to(self.device)\n",
    "\n",
    "        self.net_input_saved = self.net_input.clone().to(self.device)\n",
    "        self.noise = self.net_input.clone().to(self.device)\n",
    "        \n",
    "        # Initialize Iterations\n",
    "        self.i=0\n",
    "        self.sample_count=0\n",
    "\n",
    "        # bon voyage\n",
    "        print('Starting optimization with SGLD')\n",
    "\n",
    "    def forward(self, net_input_saved):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        occurs in the closure function in this implementation\n",
    "        \"\"\"\n",
    "        if self.reg_noise_std > 0:\n",
    "            self.net_input = self.net_input_saved + (self.noise.normal_() * self.reg_noise_std)\n",
    "            return self.model(self.net_input)\n",
    "        else:\n",
    "            return self.model(net_input_saved)\n",
    "\n",
    "    def update_burnin(self,out_np):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        check if we should end the burnin phase\n",
    "        \"\"\"\n",
    "        # update img collection\n",
    "        v_img_np = out_np.reshape(-1)\n",
    "        self.update_img_collection(v_img_np)\n",
    "        img_collection = self.get_img_collection()\n",
    "\n",
    "        if len(img_collection) >= self.buffer_size:\n",
    "            # update variance and var history\n",
    "            ave_img = np.mean(img_collection, axis=0)\n",
    "            variance = [self.MSE(ave_img, tmp) for tmp in img_collection]\n",
    "            self.cur_var = np.mean(variance)\n",
    "            self.variance_history.append(self.cur_var)\n",
    "            self.check_stop(self.cur_var, self.i)\n",
    "    \n",
    "    def backtracking(self, psrn_noisy, total_loss):\n",
    "        \"\"\"\n",
    "        Componenet of closure function\n",
    "        backtracking to prevent oscillation if the PSNR is fluctuating\n",
    "        \"\"\"\n",
    "        if self.roll_back and self.i % self.show_every:\n",
    "            if psrn_noisy - self.psrn_noisy_last < -5: \n",
    "                print('Falling back to previous checkpoint.')\n",
    "                for new_param, net_param in zip(self.last_net, self.model.parameters()):\n",
    "                    net_param.detach().copy_(new_param.cuda())\n",
    "                return total_loss*0\n",
    "            else:\n",
    "                self.last_net = [x.detach().cpu() for x in self.model.parameters()]\n",
    "                self.psrn_noisy_last = psrn_noisy\n",
    "\n",
    "    def closure_sgld(self):\n",
    "        out = self.forward(self.net_input)\n",
    "\n",
    "        # compute loss\n",
    "        total_loss = self.criteria(out, self.img_noisy_torch)\n",
    "        total_loss.backward()\n",
    "        out_np = out.detach().cpu().numpy()[0]\n",
    "\n",
    "        # compute PSNR\n",
    "        psrn_noisy = compare_psnr(self.img_noisy_np, out.detach().cpu().numpy()[0])\n",
    "        psrn_gt    = compare_psnr(self.img_np, out_np)\n",
    "        self.sgld_psnr_list.append(psrn_gt)\n",
    "\n",
    "        # early burn in termination criteria\n",
    "        if not self.burnin_over:\n",
    "            self.update_burnin(out_np)\n",
    "\n",
    "        # backtracking \n",
    "        self.backtracking(psrn_noisy, total_loss)\n",
    "\n",
    "        ##########################################\n",
    "        ### Logging and SGLD mean collection #####\n",
    "        ##########################################\n",
    "        \n",
    "        if self.burnin_over and np.mod(self.i, self.MCMC_iter) == 0:\n",
    "            self.sgld_mean += out_np\n",
    "            self.sample_count += 1.\n",
    "            sgld_psnr = compare_psnr(self.img_np, self.sgld_mean / self.sample_count)\n",
    "            nni.report_intermediate_result({'loss': total_loss, 'psnr_gt': psrn_gt, 'psnr_sgld': sgld_psnr})\n",
    "\n",
    "        if self.burnin_over:\n",
    "            self.burnin_iter+=1\n",
    "            self.sgld_mean_each += out_np\n",
    "\n",
    "        elif self.cur_var is not None and not self.burnin_over:\n",
    "            nni.report_intermediate_result({'loss': total_loss, 'psnr_gt': psrn_gt, 'var': self.cur_var})\n",
    "\n",
    "        else:\n",
    "            nni.report_intermediate_result({'loss': total_loss, 'psnr_gt': psrn_gt})\n",
    "\n",
    "        self.i += 1\n",
    "        return total_loss\n",
    "\n",
    "    def add_noise(self, net):\n",
    "        \"\"\"\n",
    "        Add noise to the network parameters\n",
    "        This is the critical part of SGLD\n",
    "        \"\"\"\n",
    "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "            noise = torch.randn(n.size())*self.param_noise_sigma*self.learning_rate\n",
    "            noise = noise.type(dtype)\n",
    "            n.data = n.data + noise\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        \"\"\"\n",
    "        Oh the places you'll go\n",
    "        ---> Straight to error city calling this add_noise in the training step\n",
    "        ---> Consider using the on_train_batch_end hook? (each batch is only one iteration)\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizers()\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.closure_sgld()\n",
    "        optimizer.step()\n",
    "        self.add_noise(self.model)\n",
    "        return loss\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        \"\"\"\n",
    "        May all your dreams come true\n",
    "        \"\"\"\n",
    "        # get output by sending net_input_saved through the network\n",
    "        # compute PSNR\n",
    "        out = self.forward(self.net_input_saved)\n",
    "        out_np = out.detach().cpu().numpy()[0]\n",
    "        psrn_gt    = compare_psnr(self.img_np, out_np)\n",
    "        \n",
    "        # compute SGLD mean from MCMC samples\n",
    "        sgld_final = self.sgld_mean / self.sample_count\n",
    "        sgld_final_psnr = compare_psnr(self.img_np, sgld_final)\n",
    "        \n",
    "        # compute SGLD mean from all post burnin samples\n",
    "        self.sgld_mean_tmp = self.sgld_mean_each / self.burnin_iter\n",
    "        sgld_final_psnr_tmp = compare_psnr(self.img_np, self.sgld_mean_tmp)\n",
    "        \n",
    "        nni.report_final_result({'psnr_gt': psrn_gt, 'psnr_sgld': sgld_final_psnr, 'psnr_sgld_each': sgld_final_psnr_tmp})\n",
    "\n",
    "    def check_stop(self, current, cur_epoch):\n",
    "        \"\"\"\n",
    "        using an early stopper technique to determine when to end the burn in phase for SGLD\n",
    "        https://arxiv.org/pdf/2112.06074.pdf\n",
    "        https://github.com/sun-umn/Early_Stopping_for_DIP/blob/main/ES_WMV.ipynb\n",
    "        \"\"\"\n",
    "        if current < self.best_score:\n",
    "            self.best_score = current\n",
    "            self.best_epoch = cur_epoch\n",
    "            self.wait_count = 0\n",
    "            self.burnin_over = False\n",
    "        else:\n",
    "            self.wait_count += 1\n",
    "            self.burnin_over = self.wait_count >= self.patience\n",
    "        if self.burnin_over:\n",
    "            print(f'\\n\\nBurn-in completed at iter {self.i}; \\nStarting SGLD Mean sampling;\\n\\n')\n",
    "            self.show_every = self.MCMC_iter\n",
    "\n",
    "    def update_img_collection(self, cur_img):\n",
    "        \"\"\"\n",
    "        update the image collection for early stopping\n",
    "        \"\"\"\n",
    "        self.img_collection.append(cur_img)\n",
    "        if len(self.img_collection) > self.buffer_size:\n",
    "            self.img_collection.pop(0)\n",
    "\n",
    "    def get_img_collection(self):\n",
    "        \"\"\"\n",
    "        get the image collection for early stopping\n",
    "        \"\"\"\n",
    "        return self.img_collection\n",
    "\n",
    "    def MSE(self, x1, x2):\n",
    "        \"\"\"\n",
    "        compute the mean squared error between two images\n",
    "        \"\"\"\n",
    "        return ((x1 - x2) ** 2).sum() / x1.size\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'patience': 1000,\n",
    "        'buffer_size': 100,\n",
    "        'weight_decay': 5e-8, # this is proportionate to a 1024x1024 image\n",
    "        }\n",
    "\n",
    "optimized_params = nni.get_next_parameter()\n",
    "params.update(optimized_params)\n",
    "print(params)\n",
    "\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor \n",
    "\n",
    "# choose iterations\n",
    "num_iter = 1e4 # max iterations\n",
    "\n",
    "# get image\n",
    "# generate phantom stored in subfolder of parent directory\n",
    "resolution = 6\n",
    "max_depth = resolution - 1\n",
    "phantom = generate_phantom(resolution=resolution)\n",
    "raw_img_np = phantom.copy() # 1x64x64 np array    \n",
    "img_np = raw_img_np.copy() # 1x64x64 np array\n",
    "sigma=25/255\n",
    "# sigma = .05\n",
    "img_noisy_np = np.clip(img_np + np.random.normal(scale=sigma, size=img_np.shape), 0, 1).astype(np.float32)\n",
    "img_noisy_torch = np_to_torch(img_noisy_np).type(dtype)\n",
    "\n",
    "# reference model \n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=1, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# Create the lightning module\n",
    "module = SGLD_HPO(\n",
    "        original_np=img_np,\n",
    "        noisy_np=img_noisy_np,\n",
    "        noisy_torch=img_noisy_torch,\n",
    "        learning_rate = params['learning_rate'],\n",
    "        patience=params['patience'],\n",
    "        buffer_size=params['buffer_size'],\n",
    "        weight_decay=params['weight_decay'],\n",
    "        )\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = Trainer(\n",
    "            max_epochs=num_iter,\n",
    "            fast_dev_run=False,\n",
    "            gpus=1,\n",
    "            # accelerator=\"tpu\",\n",
    "            # devices=1,\n",
    "            checkpoint_callback=False\n",
    "            )\n",
    "\n",
    "# Initialize ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./{lightning_logs}/{logger_name}/version_{version}/checkpoints/',\n",
    "    filename='{epoch}-{step}',\n",
    "    every_n_epochs=100,\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "# Add the checkpoint callback to trainer\n",
    "trainer.callbacks.append(checkpoint_callback)\n",
    "            \n",
    "if not hasattr(trainer, 'optimizer_frequencies'):\n",
    "    trainer.optimizer_frequencies = []\n",
    "\n",
    "# Create the lighting object for evaluator\n",
    "train_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "val_loader = DataLoader(SingleImageDataset(img_noisy_np, num_iter=1), batch_size=1)\n",
    "\n",
    "lightning = Lightning(lightning_module=module, trainer=trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "lightning.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas-test-OHy8kATa-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
